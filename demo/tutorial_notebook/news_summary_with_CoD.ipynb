{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1PMZJ8vsnimRMVbD5nmDk5UMMv3eyqjbb?usp=sharing#scrollTo=s3liO0IK-dU-\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "s3liO0IK-dU-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qc-m2-yS6kA"
      },
      "source": [
        "# Cookbook: News article summary with CoD  üë®‚Äçüç≥üë©‚Äçüç≥\n",
        "Experience Yival's powerful features by trying to summarise articles with Chain of Density (CoD).\n",
        "\n",
        "Just set **OpenAI key**, and try Chain of Density **for free**! üòç"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is YiVal?**\n",
        "> YiVal is a versatile platform support customize test data, evaluation methods and enhancement strategy , all in one.\n",
        "It enpowers you to generate better results, reduce latency and decrease inference cost.\n",
        "\n",
        "**~~TL~~DR**: YiVal streamlines the **evaluation** and **enhancement** of GenAI Apps, enhance ane evaluate **everything** with ease."
      ],
      "metadata": {
        "id": "EWuP6Kd_jBMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why YiVal**\n",
        "\n",
        "\n",
        "*   Native support **Multi-modal** apps: textüìÑ + audioüéô + imageüåÉ + videoüé•\n",
        "*   **Multi-components**: which doesn't even have to be GenAI üòÅ\n",
        "*   Native **RLHF** and **RLAIF** ‚öôÔ∏è\n",
        "*   Most advanced open source **enhancement algorithms** ü™Ñ"
      ],
      "metadata": {
        "id": "SBtz6eyMjF0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction: News Summary with CoD**\n",
        "\n",
        "\n",
        "In this notebook, we'll use Yival to conduct a \"Chain of Density\" experiment for summarizing news articles, based on the paper: [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://huggingface.co/papers/2309.04269). By integrating the CoD method into Yival, we aim to evaluate the enhancer's ability in text summarization."
      ],
      "metadata": {
        "id": "t_u6HJWZjKkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Chain of Density (CoD)?\n",
        "- When generating article summaries, it is challenging to make them contain a reasonable amount of information. A good summary should be detailed and entity-centred, rather than entity-dense and difficult to understand.\n",
        "\n",
        "- CoD is a nifty technique originally designed to craft increasingly concise, yet information-rich summaries from a given content. It identifies a set of unique and salient entities in the source text at a fixed number of iteration rounds and fuses them into the previous summary without increasing the text length. Here's how it works:\n",
        "\n",
        "  CoD operates in a two-step cycle, repeated five times:\n",
        "  - Think of it as a detective, first identifying 1-3 key pieces of information - or 'entities' - from the article that are absent from the existing summary.\n",
        "  \n",
        "  - Then, like a skilled writer, it crafts a new, denser summary. This new summary is no longer than the previous one, yet it cleverly incorporates all previous details plus the newly identified entities.\n",
        "\n",
        "- CoD follows a set of guidelines to ensure the summaries are not just dense, but also stand-alone. It's like a mini masterclass in writing - promoting the use of fusion, compression, and the removal of uninformative phrases to make room for additional entities. And, it never forgets - entities from the previous summary are always retained.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/71804564/277099109-bdfc384e-bd0f-4868-9ac8-2e8cd5a6af0a.png)"
      ],
      "metadata": {
        "id": "JbCDP0DhjOO5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVP4wjWTA-J2"
      },
      "source": [
        "# Install Dependencies & Necessary Configurations‚úàÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with installing all the dependencies and make all necessary configurations in Colab!üîõ"
      ],
      "metadata": {
        "id": "4iyEnNwxgyP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install YiVal with git"
      ],
      "metadata": {
        "id": "7VljWq-2i2aY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WEFjY14-MeP"
      },
      "outputs": [],
      "source": [
        "# Clone the latest yival\n",
        "import os\n",
        "!python --version\n",
        "!rm -rf YiVal\n",
        "!git clone https://github.com/YiVal/YiVal.git\n",
        "\n",
        "# Install and config poetry\n",
        "import shutil\n",
        "!pip install poetry\n",
        "POETRY_PATH = shutil.which(\"poetry\") or (os.getenv(\"HOME\") + \"/.local/bin/poetry\")\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.dirname(POETRY_PATH)\n",
        "!poetry --version\n",
        "!poetry config virtualenvs.create true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DkmbnLmCZQG"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/YiVal\")\n",
        "!poetry install --no-ansi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have all necessary dependencies in Colab. One step left to complete!üí™\n",
        "\n"
      ],
      "metadata": {
        "id": "HNAnIP7ejd-l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M971BMpUBI-e"
      },
      "source": [
        "## Configure your OpenAI API key"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acquire your OpenAI API key from the [OpenAI platform](https://platform.openai.com/) and paste it below."
      ],
      "metadata": {
        "id": "HG_SYx5Gkd0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb2zqa0uBPAW"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY']= ''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[Optional] Change gpt-4 to gpt-3.5-turbo in config**\n",
        "\n",
        "If you don't have a GPT-4 account, you can also use GPT-3.5-turbo to complete the entire process, you just need to modify the **model_name** in the config file.\n",
        "\n",
        "For example , you can find `model_name` below\n",
        "\n",
        "```yaml\n",
        "description: Generate test data\n",
        "dataset:\n",
        "  data_generators:\n",
        "    openai_prompt_data_generator:\n",
        "      chunk_size: 100000\n",
        "      diversify: true\n",
        "      model_name: gpt-4 #Change the model_name to gpt-3.5-turbo here ü¶ÑÔ∏è\n",
        "      input_function:\n",
        "        description:\n",
        "          Given a tech startup business, generate a corresponding landing\n",
        "          page headline\n",
        "        name: headline_generation_for_business\n",
        "        parameters:\n",
        "          tech_startup_business: str\n",
        "      number_of_examples: 3\n",
        "      output_csv_path: generated_examples.csv\n",
        "  source_type: machine_generated\n",
        "```\n",
        "\n",
        "If you want to use gpt-3.5-turbo, change the `use_gpt_35_turbo` to `True` in the below cell and run it, after you save your configurations below the `/demo/configs` folder.\n",
        "\n",
        "It will autotimatically replace all `gpt-4` to `gpt-3.5-turbo` in all yamls provided by yival"
      ],
      "metadata": {
        "id": "nqe9zXtsf1p_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD3oZoj8w19S"
      },
      "outputs": [],
      "source": [
        "import os, glob, yaml\n",
        "use_gpt_35_turbo = True  #change it to True if you don't want to use gpt-4\n",
        "\n",
        "def replace_gpt4_recursive(data):\n",
        "    if isinstance(data, str):\n",
        "        return data.replace('gpt-4', 'gpt-3.5-turbo')\n",
        "    elif isinstance(data, list):\n",
        "        return [replace_gpt4_recursive(item) for item in data]\n",
        "    elif isinstance(data, dict):\n",
        "        return {key: replace_gpt4_recursive(value) for key, value in data.items()}\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "\n",
        "def replace_in_yaml_files(directory):\n",
        "    for filename in glob.glob(os.path.join(directory, '*.yml')):\n",
        "        with open(filename, 'r') as file:\n",
        "            data = yaml.safe_load(file)\n",
        "        data = replace_gpt4_recursive(data)\n",
        "        with open(filename, 'w') as file:\n",
        "            yaml.safe_dump(data, file)\n",
        "\n",
        "if use_gpt_35_turbo:\n",
        "  replace_in_yaml_files(\"/content/YiVal/demo/configs\")\n",
        "  print(\"[INFO] replace all gpt-4 to gpt-3.5-turbo. Use gpt-3.5-turbo in the coming page\")\n",
        "else:\n",
        "  print(\"[INFO] use default gpt-4\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you are fully ready. Prepare to start our journey!üöóüöó"
      ],
      "metadata": {
        "id": "1aRoD2r0lK-D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Wq1Oeb-bSY"
      },
      "source": [
        "# News Article Summary Demoüìï\n",
        "\n",
        "Let's see how to write YiVal's configuration file for news article summary with CoD. The overall pipeline is shown in the following graph:\n",
        "\n",
        "![graph](https://user-images.githubusercontent.com/71804564/277108899-4aff68cd-40c0-47aa-ab5f-55a8fa3444a1.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configs\n",
        "The configuration file outlines the setup for an experiment involving the\n",
        "summarization of news articles. It is structured with the following parts:\n",
        "\n",
        "- Dataset Configs\n",
        "\n",
        "- Custom Function\n",
        "\n",
        "- Human Rating Configs\n",
        "\n",
        "- Variation Configs\n",
        "\n",
        "- Evaluator Configs\n",
        "\n",
        "- Selection Strategy Configs\n",
        "\n",
        "- Improver Configs"
      ],
      "metadata": {
        "id": "Qz4IxMWNirBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Configs\n",
        "The data for the experiment is sourced from [HuggingFace dataset](https://huggingface.co/datasets/griffin/chain_of_density).   The configuration is shown below:\n",
        "\n",
        "```yaml\n",
        "dataset:\n",
        "  file_path: https://datasets-server.huggingface.co/rows?dataset=griffin%2Fchain_of_density&config=annotated&split=test\n",
        "  reader: huggingface_dataset_reader\n",
        "  source_type: dataset\n",
        "  reader_config:\n",
        "    example_limit: 3\n",
        "    output_mapping:\n",
        "      article: article\n",
        "```\n",
        "Fields in the configuration:\n",
        "- **file_path**: The path of the dataset. Here we use the link of the dataset as its path.\n",
        "- **reader**: The reader of the dataset. It is set to read HuggingFace datasets.\n",
        "- **source_type**: The source type of the dataset. Here \"dataset\" means the data is from a dataset file rather than generated.\n",
        "- **example_limit**: The number limit of data example used in the experiment.\n",
        "- **output_mapping**: The mapping between fields in the dataset and the fields in the output. Here the output mapping is set to map the 'article' field in the dataset to 'article' in the output."
      ],
      "metadata": {
        "id": "5yc-Hzy-nRxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Function\n",
        "In this function, we take the content of an article as input, and output the article summary generated by GPT-4 for this article.\n",
        "\n",
        "Here, we write the custom function into `summarize.py` file for later use."
      ],
      "metadata": {
        "id": "tTuPZe3JnXLt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duwwUroNgOVw"
      },
      "outputs": [],
      "source": [
        "code = '''\n",
        "import os\n",
        "\n",
        "import openai\n",
        "\n",
        "from yival.logger.token_logger import TokenLogger\n",
        "from yival.schemas.experiment_config import MultimodalOutput\n",
        "from yival.states.experiment_state import ExperimentState\n",
        "from yival.wrappers.string_wrapper import StringWrapper\n",
        "\n",
        "\n",
        "def summarize(article: str, state: ExperimentState) -> MultimodalOutput:\n",
        "    logger = TokenLogger()\n",
        "    logger.reset()\n",
        "    # Ensure you have your OpenAI API key set up\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    # Create a chat message sequence\n",
        "    messages = [{\n",
        "        \"role\":\n",
        "        \"system\",\n",
        "        \"content\":\n",
        "        str(\n",
        "            StringWrapper(\n",
        "                \"You are a robot summarizing article summaries. Please summarize the article based on the article given below. Your abstract should be informative, capture the important information in the article, and present it accurately and concisely. In addition, the summary should be easy to understand, coherent, well-structured, and well-organized. Finally, the summary of your summary should convey the main points of the article in a concise, logical, and coherent manner.\",\n",
        "                name=\"summarization\",\n",
        "                state=state\n",
        "            )\n",
        "        )\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": article\n",
        "    }]\n",
        "    # Use the chat-based completion\n",
        "    response = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
        "\n",
        "    answer = MultimodalOutput(\n",
        "        text_output=response['choices'][0]['message']['content'],\n",
        "    )\n",
        "    token_usage = response['usage']['total_tokens']\n",
        "    logger.log(token_usage)\n",
        "\n",
        "    return answer\n",
        "\n",
        "'''\n",
        "with open('/content/YiVal/demo/summarize.py', 'w') as file:\n",
        "    file.write(code)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human Rating Configs\n",
        "A human rating configuration is defined with the name 'preference'. The instructions are to rate the quality of the generated summary, on a scale from 0 to 4.\n",
        "```yaml\n",
        "human_rating_configs:\n",
        "  - name: quality\n",
        "    instructions: Rate the quality of the generated summary.\n",
        "    scale: [0, 4]\n",
        "```\n",
        "Fields in the configuration:\n",
        "- **name**: Specifies the criterion for rating.\n",
        "- **instructions**: Provides guidelines to the rater on how to evaluate the content based on the defined criterion.\n",
        "- **scale**: The rating scale, where `1` is the lowest and `5` is the highest."
      ],
      "metadata": {
        "id": "TbrGfy-RpzA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variation Configs\n",
        "Generating prompt variations is key for generating high quality summaries.\n",
        "\n",
        "These variations are different prompts for GPT-4 to generate article summary with CoD. We tell GPT-4 the basic needs of the prompts required for CoD in this configuration. It is designed to guide the summarization of news articles in a way that is engaging and informative, while also being concise and to the point.\n",
        "\n",
        "```yaml\n",
        "variations:\n",
        "  - name: CoD_prompt_var_generation\n",
        "    generator_name: openai_prompt_based_variation_generator\n",
        "    generator_config:\n",
        "      model_name: gpt-4\n",
        "      diversify: false\n",
        "      max_tokens: 7000\n",
        "      number_of_variations: 5\n",
        "      variables:\n",
        "        - ARTICLE\n",
        "      prompt:\n",
        "        - content: |-\n",
        "\n",
        "            Your mission is to build a clean, precise instruction prompt for GPT-4. This prompt will guide GPT-4 to step-by-step generate increasingly concise, entity-dense summaries of the `{ARTICLE}` as required.\n",
        "\n",
        "            The key of your instruction is that it should prompt GPT-4 to repeat the following 2 steps 5 times:\n",
        "            - Step 1: Identify 1-3 informative Entites (\";\" delimited) from the `{ARTICLE}` which are missing from the previously generated summary.\n",
        "            - Step 2: Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entites.\n",
        "            \n",
        "            A Missing Entity is:\n",
        "            - Relevent: to the main story.\n",
        "            - Specific: descriptive yet concise (5 words or fewer).\n",
        "            - Novel: not in the previous summary.\n",
        "            - Faithful: present in the `{ARTICLE}`\n",
        "            - Anywhere: located anywhere in the `{ARTICLE}`\n",
        "\n",
        "            Craft your prompt to make sure each summary use the exact same number of words.\n",
        "            The instruction should ensure that the GPT-4 will answer the final generated summary after repeating the steps 5 times.\n",
        "            Keep your output crisp: only the prompt, devoid of any extraneous content.\n",
        "\n",
        "          role: system\n",
        "        - content: |-\n",
        "\n",
        "            {ARTICLE} represent the text of article.\n",
        "\n",
        "          role: user\n",
        "      output_path: generated_cod_prompts.pkl\n",
        "```\n",
        "Variations allow for dynamic content during experiments. They are identified by a globally unique name. For example, in your code, you might reference a variation by its name, like: `variation = StringWrapper(\"hello\", 'test_experiment')`. In this config, you would define the variations associated with that name.\n",
        "\n",
        "Fields in the configuration:\n",
        "\n",
        "- **generator_name**: Represents the class name of the generator.\n",
        "- **number_of_variations**: Specifies the number of variations to be generated.\n",
        "- **prompt**: The prompts used to generate the variation.\n",
        "- **output_path**: This is the temporary storage location for the generated data. If a file at this path exists, the data is read from it.\n",
        "\n",
        "The generated variations can be printed by running the code in the cell below:\n"
      ],
      "metadata": {
        "id": "wFdvYT48p3Gc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCTU3TUNxAQ6"
      },
      "outputs": [],
      "source": [
        "code = '''\n",
        "from yival.schemas.varation_generator_configs import OpenAIPromptBasedVariationGeneratorConfig as VarConfig\n",
        "from yival.variation_generators.openai_prompt_based_variation_generator import OpenAIPromptBasedVariationGenerator as VarGenerator\n",
        "from pprint import pprint\n",
        "\n",
        "config = VarConfig(\n",
        "    model_name=\"gpt-4\",\n",
        "    number_of_variations=3,\n",
        "    diversify=False,\n",
        "    max_tokens=7000,\n",
        "    variables=['ARTICLE'],\n",
        "    prompt=\"\"\"\n",
        "            role: system\n",
        "\n",
        "            Your mission is to build a clean, precise instruction prompt for GPT-4. This prompt will guide GPT-4 to step-by-step generate increasingly concise, entity-dense summaries of the `{ARTICLE}` as required.\n",
        "\n",
        "            The key of your instruction is that it should prompt GPT-4 to repeat the following 2 steps 5 times:\n",
        "            - Step 1: Identify 1-3 informative Entites (\";\" delimited) from the `{ARTICLE}` which are missing from the previously generated summary.\n",
        "            - Step 2: Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entites.\n",
        "\n",
        "            A Missing Entity is:\n",
        "            - Relevent: to the main story.\n",
        "            - Specific: descriptive yet concise (5 words or fewer).\n",
        "            - Novel: not in the previous summary.\n",
        "            - Faithful: present in the `{ARTICLE}`\n",
        "            - Anywhere: located anywhere in the `{ARTICLE}`\n",
        "\n",
        "            Craft your prompt to make sure each summary use the exact same number of words.\n",
        "            The instruction should ensure that the GPT-4 will answer the final generated summary after repeating the steps 5 times.\n",
        "            Keep your output crisp: only the prompt, devoid of any extraneous content.\n",
        "\n",
        "            role: user\n",
        "\n",
        "            {ARTICLE} represent the text of articles\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "generator = VarGenerator(config)\n",
        "results = generator.generate_variations()\n",
        "for item in results:\n",
        "    for var in item:\n",
        "        pprint(var.asdict().get('value',None))\n",
        "        print()\n",
        "'''\n",
        "\n",
        "with open('test_variation_generator.py', 'w') as file:\n",
        "    file.write(code)\n",
        "\n",
        "!poetry run python test_variation_generator.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluator Configs\n",
        "Evaluators are really import parts in yival.\n",
        "\n",
        "According to many [recent studies](https://ar5iv.labs.arxiv.org/html/2305.01937), large language models are human-level evaluators.\n",
        "\n",
        "For this reason, we provide the openai_prompt_based_generator in Yival, which serves to evaluate generated results through LLM.\n",
        "\n",
        "In the following cell, you can see the basic construction of the openai_prompt_based_evaluator. We will provide a detailed explanation of the different criteria for LLM and guide LLM in scoring.\n",
        "\n",
        "```yaml\n",
        "evaluators:\n",
        "\n",
        "  - evaluator_type: individual\n",
        "    metric_calculators:\n",
        "      - method: AVERAGE\n",
        "    name: openai_prompt_based_evaluator\n",
        "    display_name: informative\n",
        "    prompt: |-\n",
        "\n",
        "      You are assessing a submitted answer on a given task based on a specific criterion. Here is the data:\n",
        "      - Task: Given an article, generate a concise, entity-dense summary of identical length.\n",
        "      - Please act like a reader who has read the article. Consider the following criterion to assess the quality the summary generated by the GPT-4:\n",
        "        - How informative is the summary? A good summary should be strongly relevant to the content of the article, and captures the important information in the article.\n",
        "      [Input]: {article}\n",
        "      [Result]: {raw_output}\n",
        "      Follow the criterion, evaluate the quality of the generated summary strictly:\n",
        "      A It fails to meet the criterion at all.\n",
        "      B It somewhat meets the criterion, but there is significant room for improvement.\n",
        "      C It meets the criterion to a satisfactory degree.\n",
        "      D It meets the criterion very well.\n",
        "      E It meets the criterion exceptionally well, with little to no room for improvement.\n",
        "    choices: [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "    model_name: gpt-4\n",
        "    description: \"Evaluate the informative quality of the generated summary.\"\n",
        "    scale_description: \"0-4\"\n",
        "    choice_scores:\n",
        "      A: 0\n",
        "      B: 1\n",
        "      C: 2\n",
        "      D: 3\n",
        "      E: 4\n",
        "\n",
        "  - evaluator_type: individual\n",
        "    metric_calculators:\n",
        "      - method: AVERAGE\n",
        "    name: openai_prompt_based_evaluator\n",
        "    display_name: coherent\n",
        "    prompt: |-\n",
        "\n",
        "      You are assessing a submitted answer on a given task based on a specific criterion. Here is the data:\n",
        "      - Task: Given an article, generate a concise, entity-dense summary of identical length.\n",
        "      - Please act like a reader who has read the article. Consider the following criterion to assess the quality the summary generated by the GPT-4:\n",
        "        - How coherent is the summary? A good summary should be linguistically fluent and free of grammatical errors, well-structured and well-organized.\n",
        "      [Input]: {article}\n",
        "      [Result]: {raw_output}\n",
        "      Follow the criterion, evaluate the quality of the generated summary strictly:\n",
        "      A It fails to meet the criterion at all.\n",
        "      B It somewhat meets the criterion, but there is significant room for improvement.\n",
        "      C It meets the criterion to a satisfactory degree.\n",
        "      D It meets the criterion very well.\n",
        "      E It meets the criterion exceptionally well, with little to no room for improvement.\n",
        "    choices: [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "    model_name: gpt-4\n",
        "    description: \"Evaluate the coherent quality of the generated summary.\"\n",
        "    scale_description: \"0-4\"\n",
        "    choice_scores:\n",
        "      A: 0\n",
        "      B: 1\n",
        "      C: 2\n",
        "      D: 3\n",
        "      E: 4\n",
        "\n",
        "  - evaluator_type: individual\n",
        "    metric_calculators:\n",
        "      - method: AVERAGE\n",
        "    name: openai_prompt_based_evaluator\n",
        "    display_name: attributive\n",
        "    prompt: |-\n",
        "\n",
        "      You are assessing a submitted answer on a given task based on a specific criterion. Here is the data:\n",
        "      - Task: Given an article, generate a concise, entity-dense summary of identical length.\n",
        "      - Please act like a reader who has read the article. Consider the following criterion to assess the quality the summary generated by the GPT-4:\n",
        "        - Is all the information in the summary fully attributable to the Article?\n",
        "      [Input]: {article}\n",
        "      [Result]: {raw_output}\n",
        "      Follow the criterion, evaluate the quality of the generated summary strictly:\n",
        "      A It fails to meet the criterion at all.\n",
        "      B It somewhat meets the criterion, but there is significant room for improvement.\n",
        "      C It meets the criterion to a satisfactory degree.\n",
        "      D It meets the criterion very well.\n",
        "      E It meets the criterion exceptionally well, with little to no room for improvement.\n",
        "    choices: [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "    model_name: gpt-4\n",
        "    description: \"Evaluate the attributive quality of the generated summary.\"\n",
        "    scale_description: \"0-4\"\n",
        "    choice_scores:\n",
        "      A: 0\n",
        "      B: 1\n",
        "      C: 2\n",
        "      D: 3\n",
        "      E: 4\n",
        "```\n",
        "\n",
        "Fields in the configuration:\n",
        "- **evaluator_type**: Designates the type of evaluation.\n",
        " - `all`: The evaluator considers all experiment results across all variations. It uses the elo algorithm and employs GPT-4 as the judge.\n",
        " - `individual`: The evaluator focuses solely on the current variation's results.\n",
        "- **name**: Represents the evaluator's name or identifier.\n",
        "- **prompt**: Provides the template and context for the automated evaluator to assess a given result.\n",
        "- **display_name**: Specifies the displayed criterion name on the user interface.\n",
        "- **choices**: Lists all possible rating options for the evaluator.\n",
        "- **description**: Offers a brief description of the evaluation criterion.\n",
        "- **scale_description**: Details the numeric scoring scale.\n",
        "- **choice_scores**: Maps each choice to its respective numeric score."
      ],
      "metadata": {
        "id": "k8YPNl4Cgq7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selection Strategy Configs\n",
        "You might have noticed that we support a wide variety of evaluators in Yival configurations üåü!\n",
        "\n",
        "In this case, you can assess the results from multiple aspects such as similarity, accuracy and also  latency, and token_usage , all important factors to consider ü§î.\n",
        "\n",
        "But of course, we need a selection strategy üéØ to handle the outputs from various evaluators and pick the best one. In this case, we're using the AHP_strategy with different weights configured ‚öñÔ∏è. Here's a detailed config for your reference:\n",
        "\n",
        "```yaml\n",
        "selection_strategy:\n",
        "  ahp_selection:\n",
        "    criteria:\n",
        "      - \"openai_prompt_based_evaluator: informative\"\n",
        "      - \"openai_prompt_based_evaluator: coherent\"\n",
        "      - \"openai_prompt_based_evaluator: attributive\"\n",
        "      - average_token_usage\n",
        "      - average_latency\n",
        "    criteria_maximization:\n",
        "      \"openai_prompt_based_evaluator: informative\": true\n",
        "      \"openai_prompt_based_evaluator: coherent\": true\n",
        "      \"openai_prompt_based_evaluator: attributive\": true\n",
        "      average_latency: false\n",
        "      average_token_usage: false\n",
        "    criteria_weights:\n",
        "      \"openai_prompt_based_evaluator: informative\": 0.33\n",
        "      \"openai_prompt_based_evaluator: coherent\": 0.33\n",
        "      \"openai_prompt_based_evaluator: attributive\": 0.33\n",
        "      average_latency: 0.0\n",
        "      average_token_usage: 0.0\n",
        "```\n",
        "Fields in the configuration:\n",
        "- **selection_strategy**: Represents the overarching approach for making selections.\n",
        "- **ahp_selection**: Specifies that the Analytic Hierarchy Process (AHP) algorithm is employed for the selection strategy.\n",
        "- **criteria**: Lists the evaluators and metrics that are considered during the selection process.\n",
        "- **criteria_maximization**: Indicates whether each criterion should be maximized. For instance, while a high score from the `openai_prompt_based_evaluator` is desirable (`true`), a lower `average_latency` or `average_token_usage` is preferred (`false`).\n",
        "- **criteria_weights**: Assigns a weight to each criterion, determining its importance in the overall evaluation. The weights sum up to 1, indicating the relative significance of each criterion in the final decision-making process."
      ],
      "metadata": {
        "id": "BeQJA6pdiAbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improver Configs\n",
        "YiVal means evaluate and enhance!\n",
        "\n",
        "Enhancer is the enhance part of yival , which is definitely important in yival part.\n",
        "\n",
        "We have implemented many cutting-edge enhancer algorithms in YiVal.\n",
        "\n",
        "In this demo, we will be using the [opro_enhancer](https://github.com/YiVal/YiVal/blob/master/src/yival/combination_improvers/optimize_by_prompt_improver.py#L153), which is one of the research achievements of the DeepMind team.\n",
        "\n",
        "<img width=\"579\" alt=\"opro\" src=\"https://github.com/crazycth/pictures/assets/55043304/b2589368-caca-4e8a-af5f-f2bcee70d89c\">\n",
        "\n",
        "\n",
        "In opro evolve algorihm, given the meta-prompt as the input , the LLM generates new solutions to our objective function, then new solutions and their scores are added to the meta-prompt for the next step.\n",
        "\n",
        "YiVal's architecture is perfectly suited for this iterative approach, requiring only a simple configuration file to achieve powerful enhancement.üòÅ\n",
        "\n",
        "You can find our enhancer config belowÔºö\n",
        "```yaml\n",
        "improver:\n",
        "  name: \"optimize_by_prompt_improver\"\n",
        "  model_name: \"gpt-4\"\n",
        "  max_iterations: 2\n",
        "  improve_var: [\"CoD_prompt_var_generation\"]\n",
        "  head_meta_instruction: |-\n",
        "\n",
        "    Your mission is to build a clean, precise instruction prompt for GPT-4. This prompt will guide GPT-4 to step-by-step generate increasingly concise, entity-dense summaries of the `{ARTICLE}` as required.\n",
        "\n",
        "    I already have some prompts and their evaluation results:\n",
        "\n",
        "  end_meta_instruction: |-\n",
        "\n",
        "    Give me a new prompt that is different from all pairs above, and has a evaluation value higher than any of above.\n",
        "```\n",
        "\n",
        "Fields in the configuration:\n",
        "- **name**: Specifies the identifier or the class of the improver. In this instance, `optimize_by_prompt_improver` is utilized.\n",
        "- **max_iterations**: Designates the upper limit for the number of improvement cycles. The process will not exceed 2 iterations, irrespective of other conditions.\n",
        "- **model_name**: Indicates the model to be utilized for the improvement process, which here is `gpt-4`."
      ],
      "metadata": {
        "id": "c3GPr4_TiRF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Configuration\n",
        "Based on the above configurations, we will write the final overall configuration into the config_cod.yml file."
      ],
      "metadata": {
        "id": "lTKhl2Dgi7Ol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S5-hVmkEjE2"
      },
      "outputs": [],
      "source": [
        "code = '''\n",
        "custom_function: demo.summarize.summarize\n",
        "description: CoD Experiment Config\n",
        "dataset:\n",
        "  file_path: https://datasets-server.huggingface.co/rows?dataset=griffin%2Fchain_of_density&config=annotated&split=test\n",
        "  reader: huggingface_dataset_reader\n",
        "  source_type: dataset\n",
        "  reader_config:\n",
        "    example_limit: 3\n",
        "    output_mapping:\n",
        "      article: article\n",
        "\n",
        "human_rating_configs:\n",
        "  - name: quality\n",
        "    instructions: Rate the quality of the generated summary.\n",
        "    scale: [1, 5]\n",
        "\n",
        "variations:\n",
        "  - name: CoD_prompt_var_generation\n",
        "    generator_name: openai_prompt_based_variation_generator\n",
        "    generator_config:\n",
        "      model_name: gpt-4\n",
        "      diversify: false\n",
        "      max_tokens: 7000\n",
        "      number_of_variations: 5\n",
        "      variables:\n",
        "        - ARTICLE\n",
        "      prompt:\n",
        "        - content: |-\n",
        "\n",
        "            Your mission is to build a clean, precise instruction prompt for GPT-4. This prompt will guide GPT-4 to step-by-step generate increasingly concise, entity-dense summaries of the `{ARTICLE}` as required.\n",
        "\n",
        "            The key of your instruction is that it should prompt GPT-4 to repeat the following 2 steps 5 times:\n",
        "            - Step 1: Identify 1-3 informative Entites (\";\" delimited) from the `{ARTICLE}` which are missing from the previously generated summary.\n",
        "            - Step 2: Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entites.\n",
        "\n",
        "            A Missing Entity is:\n",
        "            - Relevent: to the main story.\n",
        "            - Specific: descriptive yet concise (5 words or fewer).\n",
        "            - Novel: not in the previous summary.\n",
        "            - Faithful: present in the `{ARTICLE}`\n",
        "            - Anywhere: located anywhere in the `{ARTICLE}`\n",
        "\n",
        "            Craft your prompt to make sure each summary use the exact same number of words.\n",
        "            The instruction should ensure that the GPT-4 will answer the final generated summary after repeating the steps 5 times.\n",
        "            Keep your output crisp: only the prompt, devoid of any extraneous content.\n",
        "\n",
        "          role: system\n",
        "        - content: |-\n",
        "\n",
        "            {ARTICLE} represent the text of article.\n",
        "\n",
        "          role: user\n",
        "      output_path: generated_cod_prompts.pkl\n",
        "\n",
        "      # Variations allow for dynamic content during experiments.\n",
        "      # They are identified by a globally unique name. For example, in your code,\n",
        "      # you might reference a variation by its name, like:\n",
        "      # variation = StringWrapper(\"hello\", 'test_experiment')\n",
        "      # In this config, you would define the variations associated with that name\n",
        "\n",
        "evaluators:\n",
        "\n",
        "  - evaluator_type: individual\n",
        "    metric_calculators:\n",
        "      - method: AVERAGE\n",
        "    name: openai_prompt_based_evaluator\n",
        "    display_name: informative\n",
        "    prompt: |-\n",
        "\n",
        "      You are assessing a submitted answer on a given task based on a specific criterion. Here is the data:\n",
        "      - Task: Given an article, generate a concise, entity-dense summary of identical length.\n",
        "      - Please act like a reader who has read the article. Consider the following criterion to assess the quality the summary generated by the GPT-4:\n",
        "        - How informative is the summary? A good summary should be strongly relevant to the content of the article, and captures the important information in the article.\n",
        "      [Input]: {article}\n",
        "      [Result]: {raw_output}\n",
        "      Follow the criterion, evaluate the quality of the generated summary strictly:\n",
        "      A It fails to meet the criterion at all.\n",
        "      B It somewhat meets the criterion, but there is significant room for improvement.\n",
        "      C It meets the criterion to a satisfactory degree.\n",
        "      D It meets the criterion very well.\n",
        "      E It meets the criterion exceptionally well, with little to no room for improvement.\n",
        "    choices: [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "    model_name: gpt-4\n",
        "    description: \"Evaluate the informative quality of the generated summary.\"\n",
        "    scale_description: \"0-4\"\n",
        "    choice_scores:\n",
        "      A: 0\n",
        "      B: 1\n",
        "      C: 2\n",
        "      D: 3\n",
        "      E: 4\n",
        "\n",
        "  - evaluator_type: individual\n",
        "    metric_calculators:\n",
        "      - method: AVERAGE\n",
        "    name: openai_prompt_based_evaluator\n",
        "    display_name: coherent\n",
        "    prompt: |-\n",
        "\n",
        "      You are assessing a submitted answer on a given task based on a specific criterion. Here is the data:\n",
        "      - Task: Given an article, generate a concise, entity-dense summary of identical length.\n",
        "      - Please act like a reader who has read the article. Consider the following criterion to assess the quality the summary generated by the GPT-4:\n",
        "        - How coherent is the summary? A good summary should be linguistically fluent and free of grammatical errors, well-structured and well-organized.\n",
        "      [Input]: {article}\n",
        "      [Result]: {raw_output}\n",
        "      Follow the criterion, evaluate the quality of the generated summary strictly:\n",
        "      A It fails to meet the criterion at all.\n",
        "      B It somewhat meets the criterion, but there is significant room for improvement.\n",
        "      C It meets the criterion to a satisfactory degree.\n",
        "      D It meets the criterion very well.\n",
        "      E It meets the criterion exceptionally well, with little to no room for improvement.\n",
        "    choices: [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "    model_name: gpt-4\n",
        "    description: \"Evaluate the coherent quality of the generated summary.\"\n",
        "    scale_description: \"0-4\"\n",
        "    choice_scores:\n",
        "      A: 0\n",
        "      B: 1\n",
        "      C: 2\n",
        "      D: 3\n",
        "      E: 4\n",
        "\n",
        "  - evaluator_type: individual\n",
        "    metric_calculators:\n",
        "      - method: AVERAGE\n",
        "    name: openai_prompt_based_evaluator\n",
        "    display_name: attributive\n",
        "    prompt: |-\n",
        "\n",
        "      You are assessing a submitted answer on a given task based on a specific criterion. Here is the data:\n",
        "      - Task: Given an article, generate a concise, entity-dense summary of identical length.\n",
        "      - Please act like a reader who has read the article. Consider the following criterion to assess the quality the summary generated by the GPT-4:\n",
        "        - Is all the information in the summary fully attributable to the Article?\n",
        "      [Input]: {article}\n",
        "      [Result]: {raw_output}\n",
        "      Follow the criterion, evaluate the quality of the generated summary strictly:\n",
        "      A It fails to meet the criterion at all.\n",
        "      B It somewhat meets the criterion, but there is significant room for improvement.\n",
        "      C It meets the criterion to a satisfactory degree.\n",
        "      D It meets the criterion very well.\n",
        "      E It meets the criterion exceptionally well, with little to no room for improvement.\n",
        "    choices: [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "    model_name: gpt-4\n",
        "    description: \"Evaluate the attributive quality of the generated summary.\"\n",
        "    scale_description: \"0-4\"\n",
        "    choice_scores:\n",
        "      A: 0\n",
        "      B: 1\n",
        "      C: 2\n",
        "      D: 3\n",
        "      E: 4\n",
        "\n",
        "selection_strategy:\n",
        "  ahp_selection:\n",
        "    criteria:\n",
        "      - \"openai_prompt_based_evaluator: length\"\n",
        "      - \"openai_prompt_based_evaluator: informative\"\n",
        "      - \"openai_prompt_based_evaluator: coherent\"\n",
        "      - \"openai_prompt_based_evaluator: attributive\"\n",
        "      - average_token_usage\n",
        "      - average_latency\n",
        "    criteria_maximization:\n",
        "      \"openai_prompt_based_evaluator: length\": true\n",
        "      \"openai_prompt_based_evaluator: informative\": true\n",
        "      \"openai_prompt_based_evaluator: coherent\": true\n",
        "      \"openai_prompt_based_evaluator: attributive\": true\n",
        "      average_latency: false\n",
        "      average_token_usage: false\n",
        "    criteria_weights:\n",
        "      \"openai_prompt_based_evaluator: length\": 0.25\n",
        "      \"openai_prompt_based_evaluator: informative\": 0.25\n",
        "      \"openai_prompt_based_evaluator: coherent\": 0.25\n",
        "      \"openai_prompt_based_evaluator: attributive\": 0.25\n",
        "      average_latency: 0.0\n",
        "      average_token_usage: 0.0\n",
        "\n",
        "improver:\n",
        "  name: \"optimize_by_prompt_improver\"\n",
        "  model_name: \"gpt-4\"\n",
        "  max_iterations: 2\n",
        "  improve_var: [\"CoD_prompt_var_generation\"]\n",
        "  head_meta_instruction: |-\n",
        "\n",
        "    Your mission is to build a clean, precise instruction prompt for GPT-4. This prompt will guide GPT-4 to generate increasingly concise, entity-dense summaries of the article as required.\n",
        "\n",
        "    I already have some prompts and their evaluation results:\n",
        "\n",
        "  end_meta_instruction: |-\n",
        "\n",
        "    Give me a new prompt that is different from all pairs above, and has a evaluation value higher than any of above.\n",
        "\n",
        "'''\n",
        "\n",
        "with open('/content/YiVal/demo/configs/config_cod.yml', 'w') as file:\n",
        "    file.write(code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayuaYgH7Sqr-"
      },
      "source": [
        "# Configure your Ngrok token\n",
        "Our current ngrok authtoken only supports one public session at a time. If it's being used by others or if you're using it to run multiple Colabs at once, you might bump into a Network error. To avoid this, we suggest getting your own ngrok authtoken for your Colab notebooks. It's easy and free to get your own authtoken from ngrok.\n",
        "\n",
        "Here's how to do it:\n",
        "- If you don't have a ngrok account yet, head over to https://dashboard.ngrok.com/login to sign up.\n",
        "- Once you're logged in, you can grab your authtoken at https://dashboard.ngrok.com/get-started/your-authtoken.\n",
        "\n",
        "Prior to initiating a new demo, ensure that all other applications utilizing ngrok within Colab have been terminated via the `Connect -> Manage Sessions` pathway. You can check and manage your sessions as follow picture.\n",
        "\n",
        "<img src=\"https://github.com/uni-zhuan/uni_CDN/blob/master/picture/Yival/iShot_2023-10-12_22.51.49.png?raw=true\" width=\"80%\" height=\"50%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96t7abbdSqT_"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "os.environ['ngrok']='true'\n",
        "public_url = ngrok.connect(addr = 8501)\n",
        "!poetry run ngrok config add-authtoken {your ngrok authtoken}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOa6CXBYHxi_"
      },
      "source": [
        "# Yival!\n",
        "\n",
        "Access Yival from NgrokTunnel public URL and check the result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knhfDk3ZHNGm"
      },
      "outputs": [],
      "source": [
        "!poetry run yival run /content/YiVal/demo/configs/config_cod.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCso4fmyA2h_"
      },
      "source": [
        "# Results Example\n",
        "After launching Yival on web, you can easily check and analysis your experiment results and detailed results as following pictures."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improvements by YiVal\n",
        "With yival's incrediable ability to streamlines the evaluation and enhancement of AIGC:\n",
        "* Using **3** articles points generated by GPT-4 for evaluation, after two rounds of enhancement:\n",
        " - The coherent score increased by **20.03%**!üöÄ\n",
        " - The attributive score increased by **25.18%!**üöÄ\n",
        " - The average token usage from **2054.6 -> 1473.4(-28.3%)**!"
      ],
      "metadata": {
        "id": "4RGoCmqYjVZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Results\n",
        "Here, we utilize 3 news articles and to obtain and evaluate responses. Here's the test results by the improver:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/71804564/277123610-a8414c08-9d29-497e-ae2b-58c987c1ff17.png)"
      ],
      "metadata": {
        "id": "huSSNFnY1dG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inteactive mode\n",
        "Moreover, we offer an interactive mode that allows you to generate headlines and evaluation results based on user input parameters and combinations. This gives you the freedom to explore new ideas using prompts that have been evaluated and enhanced.\n",
        "\n",
        "**Here are headlines that our generation bot created for Yival in interactive mode!** üòÜ\n",
        "![image](https://user-images.githubusercontent.com/71804564/277114832-391da628-9fb6-45c6-9376-b776cbbe943a.png)"
      ],
      "metadata": {
        "id": "tSVQcExv2Wrc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}