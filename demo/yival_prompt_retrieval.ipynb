{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# YiVal Prompt Retrieval ‚öôÔ∏è ‚öôÔ∏è\n",
        "\n",
        "### **What is YiVal?**\n",
        "> YiVal is a versatile platform support customize test data, evaluation methods and enhancement strategy , all in one.\n",
        "It enpowers you to generate better results, reduce latency and decrease inference cost.\n",
        "\n",
        "**~~TL~~DR**: YiVal streamlines the **evaluation** and **enhancement** of GenAI Apps, enhance ane evaluate **everything** with ease.\n",
        "\n",
        "### **Why YiVal**\n",
        "\n",
        "\n",
        "*   Native support **Multi-modal** apps: textüìÑ + audioüéô + imageüåÉ + videoüé•\n",
        "*   **Multi-components**: which doesn't even have to be GenAI üòÅ\n",
        "*   Native **RLHF** and **RLAIF** ‚öôÔ∏è\n",
        "*   Most advanced open source **enhancement algorithms** ü™Ñ\n"
      ],
      "metadata": {
        "id": "meBhw4yt7on-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install the latest yival with git**\n",
        "\n",
        "We provide two ways of yival install\n",
        "\n",
        "1. install with pip\n",
        "```\n",
        "pip install yival\n",
        "```\n",
        "\n",
        "2. Developer Mode: The latest yival\n",
        "```\n",
        "git clone https://github.com/YiVal/YiVal.git\n",
        "poetry config virtualenvs.create true\n",
        "poetry install\n",
        "```\n",
        "\n",
        "here we install with poetry , you can find the detail below\n",
        "* install poetry in colab environment\n",
        "* install yival with poetry"
      ],
      "metadata": {
        "id": "im-QxNyu8Ogw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the latest yival\n",
        "import os\n",
        "!python --version\n",
        "!rm -rf YiVal\n",
        "!git clone -b -b stable https://github.com/YiVal/YiVal.git\n",
        "\n",
        "# install and config poetry\n",
        "import shutil\n",
        "!pip install poetry\n",
        "POETRY_PATH = shutil.which(\"poetry\") or (os.getenv(\"HOME\") + \"/.local/bin/poetry\")\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.dirname(POETRY_PATH)\n",
        "!poetry --version\n",
        "!poetry config virtualenvs.create true"
      ],
      "metadata": {
        "id": "E9mPC0_e7tdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/YiVal\")\n",
        "!poetry install --no-ansi"
      ],
      "metadata": {
        "id": "P2AaqXXi8otO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configure your OpenAI API key**\n",
        "\n",
        "In yival, most of time we will use chatgpt as data_generator, evaluator , improver...\n",
        "\n",
        "It's really amazing to find that llm is so powerful\n",
        "\n",
        "In some of the demos provided by Yival, we use GPT-4 for data generation, result evaluation, and so on, because GPT-4 has stronger **reasoning** capabilities.\n"
      ],
      "metadata": {
        "id": "R1fq7ym38sde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "2-jFppjH8tcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Optional] Change gpt-4 to gpt-3.5-turbo in config**\n",
        "\n",
        "If you don't have a GPT-4 account, you can also use GPT-3.5-turbo to complete the entire process, you just need to modify the **model_name** in the config file.\n",
        "\n",
        "For example , you can find `model_name` below\n",
        "\n",
        "```yaml\n",
        "description: Generate test data\n",
        "dataset:\n",
        "  data_generators:\n",
        "    openai_prompt_data_generator:\n",
        "      chunk_size: 100000\n",
        "      diversify: true\n",
        "      model_name: gpt-4 #Change the model_name to gpt-3.5-turbo here ü¶ÑÔ∏è\n",
        "      input_function:\n",
        "        description:\n",
        "          Given a tech startup business, generate a corresponding landing\n",
        "          page headline\n",
        "        name: headline_generation_for_business\n",
        "        parameters:\n",
        "          tech_startup_business: str\n",
        "      number_of_examples: 3\n",
        "      output_csv_path: generated_examples.csv\n",
        "  source_type: machine_generated\n",
        "```\n",
        "\n",
        "If you want to use gpt-3.5-turbo, change the `use_gpt_35_turbo` to `True` in the below cell and run it.\n",
        "\n",
        "It will autotimatically replace all `gpt-4` to `gpt-3.5-turbo` in all yamls provided by yival"
      ],
      "metadata": {
        "id": "a9XFFUN98zvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, yaml\n",
        "use_gpt_35_turbo = True  #change it to True if you don't want to use gpt-4\n",
        "\n",
        "def replace_gpt4_recursive(data):\n",
        "    if isinstance(data, str):\n",
        "        return data.replace('gpt-4', 'gpt-3.5-turbo')\n",
        "    elif isinstance(data, list):\n",
        "        return [replace_gpt4_recursive(item) for item in data]\n",
        "    elif isinstance(data, dict):\n",
        "        return {key: replace_gpt4_recursive(value) for key, value in data.items()}\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "\n",
        "def replace_in_yaml_files(directory):\n",
        "    for filename in glob.glob(os.path.join(directory, '*.yml')):\n",
        "        with open(filename, 'r') as file:\n",
        "            data = yaml.safe_load(file)\n",
        "        data = replace_gpt4_recursive(data)\n",
        "        with open(filename, 'w') as file:\n",
        "            yaml.safe_dump(data, file)\n",
        "\n",
        "if use_gpt_35_turbo:\n",
        "  replace_in_yaml_files(\"/content/YiVal/demo/configs\")\n",
        "  print(\"[INFO] replace all gpt-4 to gpt-3.5-turbo. Use gpt-3.5-turbo in the coming page\")\n",
        "else:\n",
        "  print(\"[INFO] use default gpt-4\")\n"
      ],
      "metadata": {
        "id": "fOv7dqpT80NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simplifying Prompt Generation for LLMs with Retrieval Methods\n",
        "\n",
        "Working with Large Language Models (LLMs) like ChatGPT often involves feeding\n",
        "them prompts: short texts that give the model direction on how to respond.\n",
        "Manually crafting these prompts can be tedious. Wouldn't it be convenient if we\n",
        "could automate this process, ensuring each prompt is contextually relevant?\n",
        "Let's explore a method that does just that, using the Yival framework and the\n",
        "FAISS vector database.\n",
        "\n",
        "## **Why Automate Prompt Generation?**\n",
        "\n",
        "Think of prompts as questions or instructions you give to ChatGPT. The more\n",
        "precise the instruction, the better the model's answer. But crafting a new\n",
        "instruction for every unique scenario is time-consuming. If we could automate\n",
        "this, not only would it save time, but it'd also ensure the AI's responses are\n",
        "consistently relevant.\n",
        "\n",
        "## **Storing Prompts: FAISS to the Rescue**\n",
        "\n",
        "We source a wide variety of prompts from [awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts).\n",
        "To quickly find the best prompt for any situation:\n",
        "\n",
        "- **We use FAISS**: This tool helps store prompts in a way that makes them quick\n",
        "  to search and retrieve.\n",
        "- **We turn prompts into 'vectors'**: By transforming prompts into a mathematical\n",
        "  format (vectors), we can easily find the most fitting one for a given situation.\n",
        "\n",
        "## **How Yival Helps in Retrieval**\n",
        "\n",
        "Yival is like a toolbox that simplifies AI-related experimentation.\n",
        "When integrated with FAISS, it streamlines the process of fetching the right prompt.\n",
        "\n",
        "### **The Main Steps**\n",
        "\n",
        "1. **Find Matching Prompts**: Based on a given situation, Yival searches the FAISS\n",
        "    database to find similar prompts.\n",
        "2. **Refine with GPT**: Sometimes, the initially found prompts might not be perfect.\n",
        "    So, we use GPT to rerank them, ensuring we pick the most suitable one.\n",
        "\n",
        "## **Putting It All Together**\n",
        "\n",
        "With our setup, generating a prompt becomes straightforward. For ChatGPT,\n",
        "here's a simple example of how we might use a generated prompt:\n",
        "\n",
        "```python\n",
        "# Create a chat message sequence\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": str(StringWrapper(\"\", name=\"prompt\")) + f'\\n{input}'\n",
        "}]\n",
        "# Get a response from ChatGPT\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\", messages=messages\n",
        ")\n",
        "```\n",
        "\n",
        "Notice that we didn't need to provide a predefined prompt. The system took care\n",
        "of it!\n",
        "\n",
        "## **A Peek at the Process**\n",
        "\n",
        "### **Visual Flow**\n",
        "\n",
        "<img src=\"https://uninaruto.oss-cn-shanghai.aliyuncs.com/img/20230904154944.png\" width=\"80%\" >\n",
        "\n",
        "\n",
        "This flowchart will give you a bird's-eye view of how everything connects, from the moment we receive a use-case to generating the perfect prompt.\n",
        "\n",
        "### **Prompts Retrivel Yaml**\n",
        "\n",
        "```\n",
        "custom_function: demo.auto_prompt_bot.reply\n",
        "dataset:\n",
        "  source_type: user_input\n",
        "description: Basic config for interactive mode\n",
        "evaluators: []\n",
        "\n",
        "custom_variation_generators:\n",
        "  retrivel_variation_generator:\n",
        "    class: demo.prompts_retrivel.retrivel_variation_generator.RetrivelVariationGenerator\n",
        "    config_cls: demo.prompts_retrivel.retrivel_variation_generator_config.RetrivelVariationGeneratorConfig\n",
        "\n",
        "variations:\n",
        "  - name: prompt\n",
        "    generator_config:\n",
        "      use_case: \"travel guide\"\n",
        "    generator_name: retrivel_variation_generator\n",
        "```\n",
        "#### Dataset\n",
        "The experiment sources its data from user input, which is from interactive mode in dash app.\n",
        "\n",
        "#### Custom Function\n",
        "The custom function `demo.auto_prompt_bot.reply` is utilized in this experiment. This function takes a string 'input' as parameter, and generates a response using OpenAI's gpt-4 model.\n",
        "\n",
        "#### Variations\n",
        "In this experiment, we uses `retrivel_variation_generator` to generate a prompt related to `travel guide` use case.\n",
        "### **Results in Real-Time**\n",
        "\n",
        "![Output Screenshot](https://github.com/YiVal/YiVal/assets/1544154/35216786-ac3c-4884-8818-68647511228d)\n",
        "\n",
        "Here's an example of what the system's output looks like in action. This gives\n",
        "you an idea of the kind of prompts it can generate and how ChatGPT might respond.\n",
        "\n",
        "## **In Conclusion**\n",
        "\n",
        "The world of AI is vast and sometimes complex. But tools like Yival and FAISS,\n",
        "when combined with LLMs like ChatGPT, can make tasks like prompt generation much\n",
        "simpler. By automating this process, we're taking a step towards more efficient\n",
        "and context-aware AI interactions.\n",
        "\n",
        "You can review the full code [here](https://github.com/YiVal/YiVal/tree/master/demo/prompts_retrivel)\n"
      ],
      "metadata": {
        "id": "hb4GJWxD88Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure your Ngrok token\n",
        "Our current ngrok authtoken only supports one public session at a time. If it's being used by others or if you're using it to run multiple Colabs at once, you might bump into a Network error. To avoid this, we suggest getting your own ngrok authtoken for your Colab notebooks. It's easy and free to get your own authtoken from ngrok.\n",
        "\n",
        "Here's how to do it:\n",
        "- If you don't have a ngrok account yet, head over to https://dashboard.ngrok.com/login to sign up.\n",
        "- Once you're logged in, you can grab your authtoken at https://dashboard.ngrok.com/get-started/your-authtoken.\n",
        "\n",
        "Prior to initiating a new demo, ensure that all other applications utilizing ngrok within Colab have been terminated via the `Connect -> Manage Sessions` pathway. You can check and manage your sessions as follow picture.\n",
        "\n",
        "<img src=\"https://github.com/uni-zhuan/uni_CDN/blob/master/picture/Yival/iShot_2023-10-12_22.51.49.png?raw=true\" width=\"80%\" height=\"50%\">"
      ],
      "metadata": {
        "id": "ekmWwp6u9AOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['ngrok']='true'\n",
        "!poetry run ngrok config add-authtoken 2UK3G7MKgDqCqDnu36njaaE02bZ_7FqvcqBke5hbpgHjizoo7"
      ],
      "metadata": {
        "id": "K3YZuRly88oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YIVALÔºÅ\n",
        "\n",
        "Test from the interactive mode page on the web, input the parameters you want (such as San Francisco in our example) and select combinations. Click run, then wait to check the corresponding results."
      ],
      "metadata": {
        "id": "Q8SO70VJ9TVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!poetry run yival run /content/YiVal/demo/configs/config_prompts_retrivel.yml"
      ],
      "metadata": {
        "id": "dFVc3rTB-ERM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}