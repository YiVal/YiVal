{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"<p>          YiVal      <p>\u26a1 Auto Prompting \u26a1</p> </p> <p>\ud83d\udc49 Follow us:  | </p> <p>\ud83d\udc49 Sponsored by Discord AIGC community: </p> <p> </p>"},{"location":"#what-is-yival","title":"What is YiVal?","text":"<p>YiVal: Your Automatic Prompt Engineering Assistant for GenAI Applications YiVal is a state-of-the-art tool designed to streamline the tuning process for your GenAI app prompts and ANY configs in the loop. With YiVal, manual adjustments are a thing of the past. This data-driven and evaluation-centric approach ensures optimal prompts, precise RAG configurations, and fine-tuned model parameters. Empower your applications to achieve enhanced results, reduce latency, and minimize inference costs effortlessly with YiVal!</p> <p>Problems YiVal trying to tackle:</p> <ol> <li>Prompt Development Challenge: \"I can't create a better prompt. A score of 60    for my current prompt isn't helpful at all\ud83e\udd14.\"</li> <li>Fine-tuning Difficulty: \"I don't know how to fine-tune; the terminology and    numerous fine-tune algorithms are overwhelming\ud83d\ude35.\"</li> <li>Confidence and Scalability: \"I learned tutorials to build agents from Langchain    and LlamaIndex, but am I doing it right? Will the bot burn through my money    when I launch? Will users like my GenAI app\ud83e\udd2f?\"</li> <li>Models and Data Drift: \"Models and data keep changing; I worry a well-performing    GenAI app now may fail later\ud83d\ude30.\"</li> <li>Relevant Metrics and Evaluators: \"Which metrics and evaluators should I focus    on for my use case\ud83d\udcca?\"</li> </ol> <p>Check out our quickstart guide!</p> <p></p>"},{"location":"#link-to-demo","title":"Link to demo","text":"<p>Tiktok title autotune</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#docker-runtime","title":"Docker Runtime","text":"<p>Install Docker and pull ourimage on DockerHub:</p> <pre><code>docker pull yival/release:latest\n</code></pre> <p>Run our image:</p> <pre><code>docker run --it yival/release:latest\n</code></pre> <p>VSCode with Docker extension is recommended for running and developments. If you are developer using GPU with Pytorch, or need jupyter lab for data science:</p> <pre><code>docker pull yival/release:cu12_torch_jupyter\ndocker run --gpus all --it -p 8888:8888 yival/release:cu12_torch_jupyter\n</code></pre>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python Version: Ensure you have <code>Python 3.10</code> or later installed.</li> <li>OpenAI API Key: Obtain an API key from OpenAI. Once you have the key, set   it as an environment variable named <code>OPENAI_API_KEY</code>.</li> </ul>"},{"location":"#installation-methods","title":"Installation Methods","text":""},{"location":"#using-pip-recommended-for-users","title":"Using pip (Recommended for Users)","text":"<p>Install the <code>yival</code> package directly using pip:</p> <pre><code>pip install yival\n</code></pre>"},{"location":"#development-setup-using-poetry","title":"Development Setup Using Poetry","text":"<p>If you're looking to contribute or set up a development environment:</p> <ol> <li>Install Poetry: If you haven't already, install Poetry.</li> <li>Clone the Repository, or use CodeSpace:</li> </ol> <p>2.1 Use CodeSpace    The easiest way to get YiVal enviornment. Click below to use the GitHub Codespace, then go to the next step.</p> <p></p> <p>2.2 Clone the Repository</p> <pre><code>git clone https://github.com/YiVal/YiVal.git\ncd YiVal\n</code></pre> <ol> <li>Setup with Poetry: Initialize the Python virtual environment and install    dependencies using Poetry. Make sure to run the below cmd in <code>/YiVal</code> directory:</li> </ol> <pre><code>poetry install --sync\n</code></pre>"},{"location":"#trying-out-yival","title":"Trying Out YiVal","text":"<p>After setting up, you can quickly get started with YiVal by generating datasets of random tech startup business names.</p>"},{"location":"#steps-to-run-your-first-yival-program","title":"Steps to Run Your First YiVal Program","text":"<ol> <li>Navigate to the yival Directory:</li> </ol> <pre><code>cd /YiVal/src/yival\n</code></pre> <ol> <li>Set OpenAI API Key: Replace <code>$YOUR_OPENAI_API_KEY</code> with your    actual OpenAI API key.</li> </ol> <p>On macOS or Linux systems,</p> <pre><code>export OPENAI_API_KEY=$YOUR_OPENAI_API_KEY\n</code></pre> <p>On Windows systems,</p> <pre><code>setx OPENAI_API_KEY $YOUR_OPENAI_API_KEY\n</code></pre> <ol> <li>Define YiVal Configuration:    Create a configuration file named <code>config_data_generation.yml</code> for automated    test dataset generation with the following content:</li> </ol> <pre><code>description: Generate test data\ndataset:\n  data_generators:\n    openai_prompt_data_generator:\n      chunk_size: 100000\n      diversify: true\n      model_name: gpt-4\n      input_function:\n        description: # Description of the function\n          Given a tech startup business, generate a corresponding landing\n          page headline\n        name: headline_generation_for_business\n        parameters:\n          tech_startup_business: str # Parameter name and type\n      number_of_examples: 3\n      output_csv_path: generated_examples.csv\n  source_type: machine_generated\n</code></pre> <ol> <li>Execute YiVal:    Run the following command from within the <code>/YiVal/src/yival</code> directory:</li> </ol> <pre><code>yival run config_data_generation.yml\n</code></pre> <ol> <li>Check the Generated Dataset:    The generated test dataset will be stored in <code>generated_examples.csv</code>.</li> </ol> <p>Please refer to YiVal Docs Page for more details about YiVal!</p>"},{"location":"#demo","title":"Demo","text":"<p>Demo Video</p> Use Case Demo Supported Features Github Link Video Demo Link \ud83d\udc2f Craft your AI story with ChatGPT and MidJourney Multi-modal support: Design an AI-powered narrative using YiVal's multi-modal support of simultaneous text and images. It supports native and seamless Reinforcement Learning from Human Feedback(RLHF) and Reinforcement Learning from AI Feedback(RLAIF). Please watch the video above for this use case. \ud83c\udf1f Evaluate performance of multiple LLMs with your own Q&amp;A test dataset Convenientlyevaluate and compare performance of your model of choice against 100+ models, thanks to LiteLLM. Analyze model performance benchmarks tailored to your customized test data or use case. \ud83d\udd25 Startup Company Headline Generation Bot Streamline generation of headlines for your startup with automated test datacreation, prompt crafting, results evaluation, and performance enhancement via GPT-4. \ud83e\uddf3 Build a Customized Travel Guide Bot Leverageautomated prompts inspired by the travel community's most popular suggestions, such as those from awesome-chatgpt-prompts. \ud83d\udcd6 Build a Cheaper Translator: Use GPT-3.5 to teach Llama2 to create a translator with lower inference cost UsingReplicate and GPT-3.5's test data, you can fine-tune Llama2's translation bot. Benefit from 18x savings while experiencing only a 6% performance decrease. \ud83e\udd16\ufe0f Chat with Your Favorite Characters - Dantan Ji from Till the End of the Moon Bring your favorite characters to life through automated prompt creation andcharacter script retrieval. \ud83d\udd0dEvaluate guardrails's performance in generating Python(.py) outputs Guardrails: where are my guardrails? \ud83d\ude2d <code>&lt;br&gt;</code>Yival: I am here. \u2b50\ufe0f<code>&lt;br&gt;&lt;br&gt;</code>The integrated evaluation experiment is carried out with 80 LeetCode problems in csv, using guardrail and using only GPT-4. The accuracy drops from 0.625 to 0.55 with guardrail, latency increases by 44%, and cost increases by 140%. Guardrail still has a long way to go from demo to production. \ud83c\udf68Visualize different foods around the world!\ud83c\udf71 Just give the place where the food belongs and the best season to taste it, and you can get a video of the season-specific food!\ud83e\udd29 \ud83c\udf88News article summary with CoD By integrating the\"Chain of Density\" method, evaluate the enhancer's ability in text summarization.\ud83c\udf86 Using 3 articles points generated by GPT-4 for evaluation, the coherent score increased by 20.03%, the attributive score increased by 25.18%!, the average token usage from 2054.6 -&gt; 1473.4(-28.3%) \ud83d\ude80. \ud83e\udd50 Automated TikTok Title Generation Bot With only two input lines, you can easily createconcise and polished TikTok video titles based on your desired target audience and video content summaries. This is presented by our auto-prompt feature: the process is automated, so you can input your requirements and enjoy the results hassle-free!"},{"location":"#contribution-guidelines","title":"Contribution Guidelines","text":"<p>If you want to contribute to YiVal, be sure to review the contribution guidelines. We use GitHub issues for tracking requests and bugs. Please join YiVal's discord channel for general questions and discussion. Join our collaborative community where your unique expertise as researchers and software engineers is highly valued! Contribute to our project and be a part of an innovative space where every line of code and research insight actively fuels advancements in technology, fostering a future that is intelligently connected and universally accessible.</p>"},{"location":"#contributors","title":"Contributors","text":"<p>  \ud83c\udf1f YiVal welcomes your contributions! \ud83c\udf1f<p> \ud83e\udd73 Thanks so much to all of our amazing contributors \ud83e\udd73</p>  ## Paper / Algorithm Implementation  | **Paper**                                                                         | **Author**                                                                                                                                                                                                                                   | **Topics**                  | **YiVal Contributor**             | **Data Generator**                                                                                                       | **Variation Generator**                                                                                                                       | **Evaluator**                                                                                                                                                                                                                | **Selector**                                                                                 | **Enhancer**                                                                                                                              | **Config**                                                                                            | | --------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- | --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- | | [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910) | [Yongchao Zhou](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Y), [Andrei Ioan Muresanu](https://arxiv.org/search/cs?searchtype=author&amp;query=Muresanu,+A+I), [Ziwen Han](https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+Z)            | YiVal Evolver, Auto-Prompting     |  | [OpenAIPromptDataGenerator](https://github.com/YiVal/YiVal/blob/master/src/yival/data_generators/openai_prompt_data_generator.py) | [OpenAIPromptVariationGenerator](https://github.com/YiVal/YiVal/blob/master/src/yival/variation_generators/openai_prompt_based_variation_generator.py) | [OpenAIPromptEvaluator](https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/openai_prompt_based_evaluator.py), [OpenAIEloEvaluator](https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/openai_elo_evaluator.py) | [AHPSelector](https://github.com/YiVal/YiVal/blob/master/src/yival/result_selectors/ahp_selection.py) | [OpenAIPromptBasedCombinationEnhancer](https://github.com/YiVal/YiVal/blob/master/src/yival/enhancers/openai_prompt_based_combination_enhancer.py) | [config](https://github.com/YiVal/YiVal/blob/master/demo/configs/headline_generation_enhance.yml)              | | [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675)        | [Tianyi Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+T), [Varsha Kishore](https://arxiv.org/search/cs?searchtype=author&amp;query=Kishore,+V), [Felix Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+F)                       | YiVal Evaluator, bertscore, rouge | [@crazycth](https://github.com/crazycth)   | -                                                                                                                              | -                                                                                                                                                   | [BertScoreEvaluator](https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/bertscore_evaluator.py)                                                                                                                          | -                                                                                                  | -                                                                                                                                               | -                                                                                                           | | [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)                                     | [Xuechen Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Xuechen%20Li), [Tianyi Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Tianyi%20Zhang), [Yann Dubois](https://arxiv.org/search/cs?searchtype=author&amp;query=Yann%20Dubois) et. al | YiVal Evaluator                   | | -                                                                                                                              | -                                                                                                                                                   | [AlpacaEvalEvaluator](https://github.com/YiVal/YiVal/blob/master/src/yival/evaluators/alpaca_eval_evaluator.py)                                                                                                                       | -                                                                                                  | -                                                                                                                                               | [config](https://github.com/YiVal/YiVal/blob/master/demo/configs/alpaca_eval.yml)                              | | [Chain of Density](https://arxiv.org/pdf/2309.04269.pdf)                                   | [Griffin Adams](https://arxiv.org/search/?query=Griffin+Adam) [Alexander R. Fabbri](https://arxiv.org/search/?query=Alexander+R.+Fabbri) et. al                                                                                                          | Prompt Engineering                |  | -                                                                                                                              | [ChainOfDensityGenerator](https://github.com/YiVal/YiVal/blob/master/src/yival/variation_generators/chain_of_density_prompt.py)                        | -                                                                                                                                                                                                                                  | -                                                                                                  | -                                                                                                                                               | [config](https://github.com/YiVal/YiVal/blob/master/demo/configs/summary_config.yml)                           | | [Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409)                    | [Chengrun Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+C) [Xuezhi Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+X) et. al                                                                                           | Prompt Engineering                | [@crazycth](https://github.com/crazycth)   | -                                                                                                                              | -                                                                                                                                                   | -                                                                                                                                                                                                                                  | -                                                                                                  | [optimize_by_prompt_enhancer](https://github.com/YiVal/YiVal/blob/opro_implement/src/yival/enhancers/optimize_by_prompt_improver.py)               | [config](https://github.com/YiVal/YiVal/blob/opro_implement/demo/configs/headline_generation_improve.yml#L174) | | [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)     | [Edward J. Hu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+E+J) [Yelong Shen](https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+Y) et. al                                                                                            | LLM Finetune                      | [@crazycth](https://github.com/crazycth)   | -                                                                                                                              | -                                                                                                                                                   | -                                                                                                                                                                                                                                  | -                                                                                                  | [sft_trainer](https://github.com/YiVal/YiVal/blob/add_finetune_module/src/yival/finetune/sft_trainer.py#L40)                                       | [config](https://github.com/YiVal/YiVal/blob/add_finetune_module/src/yival/schemas/trainer_configs.py#L48)     |"},{"location":"contributing-guide/","title":"Contributing Guide","text":""},{"location":"contributing-guide/#preparation","title":"Preparation","text":"<p>You need an Python 3.10+ environment with poetry.</p> <p>Example</p> LinuxMac OSWindowsOther <p>For example, in Ubuntu 22.04, you can run:</p> <pre><code>sudo apt install python3-pip\nsudo pip install poetry\n</code></pre> <pre><code>brew install python@3.10\n\nbrew install poetry\n# Or\npython3 -m pip install --user poetry\n</code></pre> <p>Download exe installer from python.org, or use Chocolatey :</p> <pre><code>choco install python\npython3 -m pip install poetry\n</code></pre> <p>There should be a reboot.</p> <p>See poetry document for installation guide.</p>"},{"location":"contributing-guide/#setup","title":"Setup","text":"<p>Initialize a Python virtual environment with <code>poetry</code>:</p> <pre><code>poetry install --sync\n</code></pre>"},{"location":"contributing-guide/#development","title":"Development","text":"<p>A vscode is recommended. There are some configurations in <code>.vscode/</code> of this project.</p> <p>The commands below should be executed inside <code>poetry shell</code>, or with prefix <code>poetry run</code>.</p>"},{"location":"contributing-guide/#test","title":"Test","text":"<pre><code>pytest\n</code></pre>"},{"location":"contributing-guide/#mkdocs","title":"Mkdocs","text":"<p>Preview the docs locally:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"code_reference/configs/","title":"Configs","text":""},{"location":"code_reference/configs/#yivalschemasmodel_configs","title":"yival.schemas.model_configs","text":""},{"location":"code_reference/configs/#modelprovider-objects","title":"ModelProvider Objects","text":"<pre><code>@dataclass\nclass ModelProvider()\n</code></pre> <p>Represents the details required to interact with a GebAI model.</p> <p>Arguments:</p> <ul> <li><code>api_key</code> str - The API key used for authenticating requests to the   model provider.</li> <li><code>provider_name</code> str - Name of the model provider.</li> </ul> <p></p>"},{"location":"code_reference/configs/#request-objects","title":"Request Objects","text":"<pre><code>@dataclass\nclass Request()\n</code></pre> <p>Represents the information needed to make an inference request to a GenAI model.</p> <p>Arguments:</p> <ul> <li><code>model_name</code> str - The name of the machine learning model to be used   for inference.</li> <li><code>params</code> Dict[str, Any] - Additional parameters to configure the model   for inference.</li> <li><code>prompt</code> Union[str, List[Dict[str, str]]] - The input prompt or set of   prompts to use for the inference.   Can be a simple string or a list of dictionaries, each containing   key-value pairs for more complex prompt structures.</li> </ul> <p></p>"},{"location":"code_reference/configs/#response-objects","title":"Response Objects","text":"<pre><code>@dataclass\nclass Response()\n</code></pre> <p>Represents the outcome of an inference request.</p> <p>Arguments:</p> <ul> <li><code>output</code> Any - The result generated by the GenAI model,   which can be of any type depending on the model and task.</li> </ul> <p></p>"},{"location":"code_reference/configs/#calloption-objects","title":"CallOption Objects","text":"<pre><code>@dataclass\nclass CallOption()\n</code></pre> <p>Represents call options with llm</p> <p>Arguments:</p> <ul> <li><code>temperature</code> float - hyperparameter that   controls the randomness of predictions by scaling the logits   before applying softmax.</li> </ul> <p>presence_penalty (float)</p> <p></p>"},{"location":"code_reference/configs/#yivalschemasvaration_generator_configs","title":"yival.schemas.varation_generator_configs","text":""},{"location":"code_reference/configs/#basevariationgeneratorconfig-objects","title":"BaseVariationGeneratorConfig Objects","text":"<pre><code>@dataclass\nclass BaseVariationGeneratorConfig()\n</code></pre> <p>Base configuration class for all variation generators.</p> <p></p>"},{"location":"code_reference/configs/#openaipromptbasedvariationgeneratorconfig-objects","title":"OpenAIPromptBasedVariationGeneratorConfig Objects","text":"<pre><code>@dataclass\nclass OpenAIPromptBasedVariationGeneratorConfig(BaseVariationGeneratorConfig)\n</code></pre> <p>Generate variation using chatgpt. Currently only support openai models.</p> <p></p>"},{"location":"code_reference/configs/#yivalschemasexperiment_config","title":"yival.schemas.experiment_config","text":"<p>Module for experiment configuration structures.</p> <p>This module provides data structures to capture configurations required to run an experiment.</p> <p></p>"},{"location":"code_reference/configs/#wrappervariation-objects","title":"WrapperVariation Objects","text":"<pre><code>@dataclass\nclass WrapperVariation()\n</code></pre> <p>Represents a variation within a wrapper. The value can be any type, but typical usages might include strings,  numbers, configuration dictionaries, or even custom class configurations.</p> <p></p>"},{"location":"code_reference/configs/#value_type","title":"value_type","text":"<p>e.g., \"string\", \"int\", \"float\", \"ClassA\", ...</p> <p></p>"},{"location":"code_reference/configs/#value","title":"value","text":"<p>The actual value or parameters to initialize a value</p> <p></p>"},{"location":"code_reference/configs/#instantiate","title":"instantiate","text":"<pre><code>def instantiate() -&gt; Any\n</code></pre> <p>Returns an instantiated value based on value_type and params.</p> <p></p>"},{"location":"code_reference/configs/#wrapperconfig-objects","title":"WrapperConfig Objects","text":"<pre><code>@dataclass\nclass WrapperConfig()\n</code></pre> <p>Configuration for each individual wrapper used in the experiment.</p> <p></p>"},{"location":"code_reference/configs/#outputconfig-objects","title":"OutputConfig Objects","text":"<pre><code>@dataclass\nclass OutputConfig()\n</code></pre> <p>Configuration for experiment output.</p> <p>Attributes:</p> <ul> <li>path (str): Path where the experiment output should be saved.</li> <li>formatter (Callable): Function to format the output.</li> </ul> <p></p>"},{"location":"code_reference/configs/#comparisonoutput-objects","title":"ComparisonOutput Objects","text":"<pre><code>@dataclass\nclass ComparisonOutput()\n</code></pre> <p>Result of a comparison evaluation.</p> <p>Attributes:</p> <ul> <li>better_output (str): Name of the wrapper that produced the better output.</li> <li>reason (str): Reason or metric based on which the decision was made.</li> </ul> <p></p>"},{"location":"code_reference/configs/#humanrating-objects","title":"HumanRating Objects","text":"<pre><code>@dataclass\nclass HumanRating()\n</code></pre> <p>Human rating for an output.</p> <p>Attributes:</p> <ul> <li>aspect (str): Aspect being rated.</li> <li>rating (float): Rating value.</li> <li>scale (Tuple[float, float]): Minimum and maximum value of the rating   scale.</li> </ul> <p></p>"},{"location":"code_reference/configs/#scale","title":"scale","text":"<p>Default scale from 1 to 5</p> <p></p>"},{"location":"code_reference/configs/#metric-objects","title":"Metric Objects","text":"<pre><code>@dataclass\nclass Metric()\n</code></pre> <p>Represents a metric calculated from evaluator outputs.</p> <p>Attributes:</p> <ul> <li>name (str): Name of the metric (e.g., \"accuracy\").</li> <li>value (float): Calculated value of the metric.</li> <li>description (Optional[str]): Description or details about the metric.</li> </ul> <p></p>"},{"location":"code_reference/configs/#experimentsummary-objects","title":"ExperimentSummary Objects","text":"<pre><code>@dataclass\nclass ExperimentSummary()\n</code></pre> <p>Represents the summary of an entire experiment.</p> <p>Attributes:</p> <ul> <li>aggregated_metrics (Dict[str, Dict[str, Metric]]):   A dictionary where keys are evaluator names and values are dictionaries   mapping metric names to their values.</li> <li>... (other summary attributes)</li> </ul> <p></p>"},{"location":"code_reference/configs/#context-objects","title":"Context Objects","text":"<pre><code>@dataclass\nclass Context()\n</code></pre> <p>Custom function context that will be used for evlaution</p> <p></p>"},{"location":"code_reference/configs/#multimodaloutput-objects","title":"MultimodalOutput Objects","text":"<pre><code>@dataclass\nclass MultimodalOutput()\n</code></pre> <p>Multimodal output that can include a string, a PIL Image, or both.</p> <p>Attributes:</p> <ul> <li>text_output (str): Text output for this example.</li> <li>image_output (PIL.Image.Image): Image output for this example.</li> </ul> <p></p>"},{"location":"code_reference/configs/#experimentresult-objects","title":"ExperimentResult Objects","text":"<pre><code>@dataclass\nclass ExperimentResult()\n</code></pre> <p>Result for a single example based on a specific combination of active variations across wrappers.</p> <p>Attributes:</p> <ul> <li>combination (Dict[str, str]): The combination of wrapper names and their   active variation_ids for this example.</li> <li>raw_output (Any): Raw output for this example. Support str and PILimage</li> <li>latency (float): Latency for producing the output for this example   (in milliseconds or appropriate unit).</li> <li>token_usage (int): Number of tokens used for this example.</li> <li>evaluator_outputs (List[EvaluatorOutput]): Evaluator outputs for this   example.</li> <li>human_rating (Optional[HumanRating]): Human rating for this example.</li> <li>intermediate_logs (List[str]): Logs captured during the experiment.</li> </ul> <p></p>"},{"location":"code_reference/configs/#functionmetadata-objects","title":"FunctionMetadata Objects","text":"<pre><code>@dataclass\nclass FunctionMetadata()\n</code></pre>"},{"location":"code_reference/configs/#parameters","title":"parameters","text":"<p>[(param_name, description), ...]</p> <p></p>"},{"location":"code_reference/configs/#enhanceroutput-objects","title":"EnhancerOutput Objects","text":"<pre><code>@dataclass\nclass EnhancerOutput()\n</code></pre> <p>Represents the outputs related to the \"enhancer\" component of the experiment.</p> <p>The enhancer's role is to enhance or optimize certain aspects of the experiment. This dataclass captures the results, metrics, and decisions made by the enhancer.</p> <p>Attributes:</p> <ul> <li><code>group_experiment_results</code> List[GroupedExperimentResult] - List of   grouped results after enhancement.   combination_aggregated_metrics (List[CombinationAggregatedMetrics]):   Aggregated metrics post-enhancement.</li> <li><code>original_best_combo_key</code> str - The best combination key before the   enhancer made optimizations.</li> <li><code>selection_output</code> Optional[SelectionOutput] - Output from the selection</li> </ul> <p></p>"},{"location":"code_reference/configs/#experiment-objects","title":"Experiment Objects","text":"<pre><code>@dataclass\nclass Experiment()\n</code></pre> <p>Represents the entirety of an experiment run.</p> <p>This dataclass encapsulates the results, metrics, and configurations used and generated during the experiment. It is a comprehensive view of everything related to a specific experiment run.</p> <p>Attributes:</p> <ul> <li><code>group_experiment_results</code> List[GroupedExperimentResult] - List of   results grouped by test cases.   combination_aggregated_metrics (List[CombinationAggregatedMetrics]):   Metrics aggregated for specific combinations.</li> <li><code>selection_output</code> Optional[SelectionOutput] - Output from the   selection strategy. enhancer_output (Optional[enhancerOutput]):   Output from the enhancer component, if used.</li> </ul> <p></p>"},{"location":"code_reference/configs/#yivalschemasdata_generator_configs","title":"yival.schemas.data_generator_configs","text":""},{"location":"code_reference/configs/#basedatageneratorconfig-objects","title":"BaseDataGeneratorConfig Objects","text":"<pre><code>@dataclass\nclass BaseDataGeneratorConfig()\n</code></pre> <p>Base configuration class for all data generators.</p> <p></p>"},{"location":"code_reference/configs/#openaipromptbasedgeneratorconfig-objects","title":"OpenAIPromptBasedGeneratorConfig Objects","text":"<pre><code>@dataclass\nclass OpenAIPromptBasedGeneratorConfig(BaseDataGeneratorConfig)\n</code></pre> <p>Generate test cases from prompt. Currently only support openai models.</p> <p></p>"},{"location":"code_reference/configs/#documentdatageneratorconfig-objects","title":"DocumentDataGeneratorConfig Objects","text":"<pre><code>@dataclass\nclass DocumentDataGeneratorConfig(BaseDataGeneratorConfig)\n</code></pre> <p>Generate question data from documents.</p> <p></p>"},{"location":"code_reference/configs/#yivalschemascommon_structures","title":"yival.schemas.common_structures","text":""},{"location":"code_reference/configs/#inputdata-objects","title":"InputData Objects","text":"<pre><code>@dataclass\nclass InputData()\n</code></pre> <p>Represents the input data for an experiment example.</p> <p>Attributes:</p> <ul> <li>example_id Optional[str]: A unique identifier for the example.</li> <li>content (Dict[str, Any]): A dictionary that contains all the necessary input   parameters for the custom function.</li> <li>expected_result (Optional[Any]): The expected result given the input.</li> </ul> <p></p>"},{"location":"code_reference/configs/#yivalschemasevaluator_config","title":"yival.schemas.evaluator_config","text":""},{"location":"code_reference/configs/#methodcalculationmethod-objects","title":"MethodCalculationMethod Objects","text":"<pre><code>class MethodCalculationMethod(Enum)\n</code></pre> <p>Configuration for metric calculation method.</p> <p></p>"},{"location":"code_reference/configs/#metriccalculatorconfig-objects","title":"MetricCalculatorConfig Objects","text":"<pre><code>@dataclass\nclass MetricCalculatorConfig()\n</code></pre> <p>Configuration for metric calculation.</p> <p></p>"},{"location":"code_reference/configs/#baseevaluatorconfig-objects","title":"BaseEvaluatorConfig Objects","text":"<pre><code>@dataclass\nclass BaseEvaluatorConfig()\n</code></pre> <p>Base configuration for evaluators.</p> <p></p>"},{"location":"code_reference/configs/#evaluatorconfig-objects","title":"EvaluatorConfig Objects","text":"<pre><code>@dataclass\nclass EvaluatorConfig(BaseEvaluatorConfig)\n</code></pre> <p>Configuration for custom evaluator.</p> <p></p>"},{"location":"code_reference/configs/#comparisonevaluatorconfig-objects","title":"ComparisonEvaluatorConfig Objects","text":"<pre><code>@dataclass\nclass ComparisonEvaluatorConfig(BaseEvaluatorConfig)\n</code></pre> <p>Configuration for evaluators that compare different outputs.</p> <p></p>"},{"location":"code_reference/configs/#globalevaluatorconfig-objects","title":"GlobalEvaluatorConfig Objects","text":"<pre><code>@dataclass\nclass GlobalEvaluatorConfig(BaseEvaluatorConfig)\n</code></pre> <p>Configuration for evaluators that compare based on all outputs.</p> <p></p>"},{"location":"code_reference/configs/#evaluatoroutput-objects","title":"EvaluatorOutput Objects","text":"<pre><code>@dataclass\nclass EvaluatorOutput()\n</code></pre> <p>Result of an evaluator.</p> <p></p>"},{"location":"code_reference/configs/#bertscoreevaluatorconfig-objects","title":"BertScoreEvaluatorConfig Objects","text":"<pre><code>@dataclass\nclass BertScoreEvaluatorConfig(EvaluatorConfig)\n</code></pre>"},{"location":"code_reference/configs/#indicator","title":"indicator","text":"<p>p,r,f</p> <p></p>"},{"location":"code_reference/configs/#yivalschemascombination_enhancer_configs","title":"yival.schemas.combination_enhancer_configs","text":""},{"location":"code_reference/configs/#basecombinationenhancerconfig-objects","title":"BaseCombinationEnhancerConfig Objects","text":"<pre><code>@dataclass\nclass BaseCombinationEnhancerConfig()\n</code></pre> <p>Base configuration class for all combination enhancers.</p> <p></p>"},{"location":"code_reference/configs/#yivalschemastrainer_configs","title":"yival.schemas.trainer_configs","text":""},{"location":"code_reference/configs/#basetrainerconfig-objects","title":"BaseTrainerConfig Objects","text":"<pre><code>@dataclass\nclass BaseTrainerConfig()\n</code></pre> <p>Base configuration class for all trainers</p> <p></p>"},{"location":"code_reference/configs/#loraconfig-objects","title":"LoRAConfig Objects","text":"<pre><code>@dataclass\nclass LoRAConfig()\n</code></pre> <p>LoRA config for SFT Trainer</p> <p></p>"},{"location":"code_reference/configs/#trainarguments-objects","title":"TrainArguments Objects","text":"<pre><code>@dataclass\nclass TrainArguments()\n</code></pre> <p>Train Arguments in trl trainer Parameters for training arguments details =&gt; https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py#L158</p> <p></p>"},{"location":"code_reference/configs/#sfttrainerconfig-objects","title":"SFTTrainerConfig Objects","text":"<pre><code>@dataclass\nclass SFTTrainerConfig(BaseTrainerConfig)\n</code></pre> <p>Supervised Fine-tuning trainer config</p> <p></p>"},{"location":"code_reference/configs/#yivalschemasdataset_config","title":"yival.schemas.dataset_config","text":""},{"location":"code_reference/configs/#datasetsourcetype-objects","title":"DatasetSourceType Objects","text":"<pre><code>class DatasetSourceType(Enum)\n</code></pre> <p>Enum to specify the source of dataset: USER, DATASET, or MACHINE_GENERATED.</p> <p></p>"},{"location":"code_reference/configs/#datasetconfig-objects","title":"DatasetConfig Objects","text":"<pre><code>@dataclass\nclass DatasetConfig()\n</code></pre> <p>Configuration for the dataset used in the experiment.</p> <p>Attributes:</p> <ul> <li>source_type (DatasetSourceType): Source of dataset, either directly from the user,   from a dataset, or machine-generated.</li> <li>file_path (Union[str, None]): Path to the dataset file. Relevant only if   source_type is DATASET.</li> <li>reader (Union[str, None]): Class name to process the dataset file.   Relevant only if source_type is DATASET.</li> <li>reader_config (Union[BaseReaderConfig, None]): Configuration for the reader.</li> <li>output_path (Union[str, None]): Path to store the machine-generated data. Relevant   only if source_type is MACHINE_GENERATED.</li> <li>data_generators (Optional[Dict[str, BaseDataGeneratorConfig]]): List of data_generators to generate data.   Relevant only if source_type is MACHINE_GENERATED.</li> </ul> <p></p>"},{"location":"code_reference/configs/#yivalschemaswrapper_configs","title":"yival.schemas.wrapper_configs","text":""},{"location":"code_reference/configs/#basewrapperconfig-objects","title":"BaseWrapperConfig Objects","text":"<pre><code>@dataclass\nclass BaseWrapperConfig()\n</code></pre> <p>Base configuration class for wrappers.</p> <p></p>"},{"location":"code_reference/configs/#stringwrapperconfig-objects","title":"StringWrapperConfig Objects","text":"<pre><code>@dataclass\nclass StringWrapperConfig(BaseWrapperConfig)\n</code></pre> <p>Configuration specific to the StringWrapper.</p> <p></p>"},{"location":"code_reference/configs/#yivalschemasselector_strategies","title":"yival.schemas.selector_strategies","text":""},{"location":"code_reference/configs/#yivalschemasreader_configs","title":"yival.schemas.reader_configs","text":""},{"location":"code_reference/configs/#basereaderconfig-objects","title":"BaseReaderConfig Objects","text":"<pre><code>@dataclass\nclass BaseReaderConfig()\n</code></pre> <p>Base configuration class for all readers.</p> <p></p>"},{"location":"code_reference/configs/#csvreaderconfig-objects","title":"CSVReaderConfig Objects","text":"<pre><code>@dataclass\nclass CSVReaderConfig(BaseReaderConfig)\n</code></pre> <p>Configuration specific to the CSV reader.</p>"},{"location":"code_reference/data-generators/","title":"Data Generators","text":""},{"location":"code_reference/data-generators/#yivaldata_generatorsopenai_prompt_data_generator","title":"yival.data_generators.openai_prompt_data_generator","text":"<p>This module provides an implementation for data generation using OpenAI's model.</p> <p>The primary goal of this module is to programmatically generate data examples based on a given prompt and configuration. It employs OpenAI's models to produce these examples, and offers utility functions for transforming and processing the generated data.</p> <p></p>"},{"location":"code_reference/data-generators/#openaipromptdatagenerator-objects","title":"OpenAIPromptDataGenerator Objects","text":"<pre><code>class OpenAIPromptDataGenerator(BaseDataGenerator)\n</code></pre> <p>Data generator using OpenAI's model based on provided prompts and configurations.</p> <p>This class is responsible for the generation of data examples using OpenAI's models. The generated data can be used for various purposes, including testing, simulations, and more. The nature and number of generated examples are determined by the provided configuration.</p> <p></p>"},{"location":"code_reference/data-generators/#prepare_messages","title":"prepare_messages","text":"<pre><code>def prepare_messages(all_data_content,\n                     number_of_examples) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Prepare the messages for GPT API based on configurations.</p> <p></p>"},{"location":"code_reference/data-generators/#process_outputs","title":"process_outputs","text":"<pre><code>def process_outputs(output_content: str,\n                    all_data: List[InputData],\n                    chunk: List[InputData],\n                    fixed_input: Dict[str, Any] | None = {})\n</code></pre> <p>Process the output from GPT API and update data lists.</p> <p></p>"},{"location":"code_reference/data-generators/#process_output","title":"process_output","text":"<pre><code>def process_output(output_content: str,\n                   all_data: List[InputData],\n                   chunk: List[InputData],\n                   fixed_input: Dict[str, Any] | None = {})\n</code></pre> <p>Process the output from GPT API and update data lists.</p> <p></p>"},{"location":"code_reference/data-generators/#yivaldata_generatorsbase_data_generator","title":"yival.data_generators.base_data_generator","text":"<p>This module provides a foundational architecture for programmatically generating data.</p> <p>Data generators are responsible for creating data programmatically based on certain configurations. The primary utility of these generators is in scenarios where synthetic or mock data is required, such as testing, simulations, and more. This module offers a base class that outlines the primary structure and functionalities of a data generator. It also provides methods to register new generators, retrieve existing ones, and fetch their configurations.</p> <p></p>"},{"location":"code_reference/data-generators/#basedatagenerator-objects","title":"BaseDataGenerator Objects","text":"<pre><code>class BaseDataGenerator(ABC)\n</code></pre> <p>Abstract base class for all data generators.</p> <p>This class provides a blueprint for data generators and offers methods to register new generators, retrieve registered generators, and fetch their configurations.</p> <p>Attributes:</p> <ul> <li><code>_registry</code> Dict[str, Dict[str, Any]] - A registry to keep track of   data generators.</li> <li><code>default_config</code> Optional[BaseDataGeneratorConfig] - Default   configuration for the generator.</li> </ul> <p></p>"},{"location":"code_reference/data-generators/#get_data_generator","title":"get_data_generator","text":"<pre><code>@classmethod\ndef get_data_generator(cls, name: str) -&gt; Optional[Type['BaseDataGenerator']]\n</code></pre> <p>Retrieve data generator class from registry by its name.</p> <p></p>"},{"location":"code_reference/data-generators/#get_default_config","title":"get_default_config","text":"<pre><code>@classmethod\ndef get_default_config(cls, name: str) -&gt; Optional[BaseDataGeneratorConfig]\n</code></pre> <p>Retrieve the default configuration of a data generator by its name.</p> <p></p>"},{"location":"code_reference/data-generators/#get_config_class","title":"get_config_class","text":"<pre><code>@classmethod\ndef get_config_class(cls,\n                     name: str) -&gt; Optional[Type[BaseDataGeneratorConfig]]\n</code></pre> <p>Retrieve the configuration class of a generator_info by its name.</p> <p></p>"},{"location":"code_reference/data-generators/#register_data_generator","title":"register_data_generator","text":"<pre><code>@classmethod\ndef register_data_generator(\n        cls,\n        name: str,\n        data_generator_cls: Type['BaseDataGenerator'],\n        config_cls: Optional[Type[BaseDataGeneratorConfig]] = None)\n</code></pre> <p>Register data generator class with the registry.</p> <p></p>"},{"location":"code_reference/data-generators/#generate_examples","title":"generate_examples","text":"<pre><code>@abstractmethod\ndef generate_examples() -&gt; Iterator[List[InputData]]\n</code></pre> <p>Generate data examples and return an iterator of lists containing InputData.</p> <p>This method is designed to produce data programmatically. The number and nature of data examples are determined by the generator's configuration.</p> <p>Returns:</p> <ul> <li><code>Iterator[List[InputData]]</code> - An iterator yielding lists of InputData   objects.</li> </ul> <p></p>"},{"location":"code_reference/data-generators/#generate_example_id","title":"generate_example_id","text":"<pre><code>def generate_example_id(content: str) -&gt; str\n</code></pre> <p>Generate a unique identifier for a given content string.</p> <p>Arguments:</p> <ul> <li><code>content</code> str - The content for which an ID should be generated.</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> - A unique MD5 hash derived from the content.</li> </ul> <p></p>"},{"location":"code_reference/data-generators/#yivaldata_generatorsdocument_data_generator","title":"yival.data_generators.document_data_generator","text":"<p>This module provides an implementation for generating question data from documents. Supported types of document sources include:     - plain text     - unstructured files: Text, PDF, PowerPoint, HTML, Images,          Excel spreadsheets, Word documents, Markdown, etc.     - documents from Google Drive (provide file id). Currently support only one document a time.</p> <p></p>"},{"location":"code_reference/data-generators/#documentdatagenerator-objects","title":"DocumentDataGenerator Objects","text":"<pre><code>class DocumentDataGenerator(BaseDataGenerator)\n</code></pre>"},{"location":"code_reference/data-generators/#prepare_messages_1","title":"prepare_messages","text":"<pre><code>def prepare_messages() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Prepare the messages for GPT API based on configurations.</p> <p></p>"},{"location":"code_reference/data-generators/#process_output_1","title":"process_output","text":"<pre><code>def process_output(output_content: str, all_data: List[InputData],\n                   chunk: List[InputData])\n</code></pre> <p>Process the output from GPT API and update data lists.</p>"},{"location":"code_reference/data/","title":"Data","text":""},{"location":"code_reference/data/#yivaldatacsv_reader","title":"yival.data.csv_reader","text":"<p>Read data from CSV</p> <p></p>"},{"location":"code_reference/data/#get_valid_path","title":"get_valid_path","text":"<pre><code>def get_valid_path(user_specified_path)\n</code></pre> <p>Get valid csv input path.</p> <p></p>"},{"location":"code_reference/data/#csvreader-objects","title":"CSVReader Objects","text":"<pre><code>class CSVReader(BaseReader)\n</code></pre> <p>CSVReader is a class derived from BaseReader to read datasets from CSV files.</p> <p>Attributes:</p> <ul> <li><code>config</code> CSVReaderConfig - Configuration object specifying reader   parameters.</li> <li><code>default_config</code> CSVReaderConfig - Default configuration for the   reader.</li> </ul> <p>Methods:</p> <p>init(self, config: CSVReaderConfig): Initializes the CSVReader   with a given configuration.   read(self, path: str) -&gt; Iterator[List[InputData]]: Reads the CSV   file and yields chunks of InputData.</p> <p>Notes:</p> <p>The read method checks for headers in the CSV file and raises an   error if missing.   It also checks for missing data in rows, skipping those with   missing values but logs them.   If a specified column contains expected results, it extracts those   results from the row.   Rows are read in chunks, and each chunk is yielded once its size   reaches <code>chunk_size</code>.   The class supports registering with the BaseReader using the   <code>register_reader</code> method.</p> <p>Usage:   reader = CSVReader(config)   for chunk in reader.read(path_to_csv_file):   process(chunk)</p> <p></p>"},{"location":"code_reference/data/#yivaldatabase_reader","title":"yival.data.base_reader","text":"<p>This module provides an abstract foundation for data readers.</p> <p>Data readers are responsible for reading data from various sources, and this module offers a base class to define and register new readers, retrieve existing ones, and fetch their configurations. The design encourages efficient parallel processing by reading data in chunks.</p> <p></p>"},{"location":"code_reference/data/#basereader-objects","title":"BaseReader Objects","text":"<pre><code>class BaseReader(ABC)\n</code></pre> <p>Abstract base class for all data readers.</p> <p>This class provides a blueprint for data readers and offers methods to register new readers, retrieve registered readers, and fetch their configurations.</p> <p>Attributes:</p> <ul> <li><code>_registry</code> Dict[str, Dict[str, Any]] - A registry to keep track of   data readers.</li> <li><code>default_config</code> Optional[BaseReaderConfig] - Default configuration for   the reader.</li> </ul> <p></p>"},{"location":"code_reference/data/#get_reader","title":"get_reader","text":"<pre><code>@classmethod\ndef get_reader(cls, name: str) -&gt; Optional[Type['BaseReader']]\n</code></pre> <p>Retrieve reader class from registry by its name.</p> <p></p>"},{"location":"code_reference/data/#get_default_config","title":"get_default_config","text":"<pre><code>@classmethod\ndef get_default_config(cls, name: str) -&gt; Optional[BaseReaderConfig]\n</code></pre> <p>Retrieve the default configuration of a reader by its name.</p> <p></p>"},{"location":"code_reference/data/#get_config_class","title":"get_config_class","text":"<pre><code>@classmethod\ndef get_config_class(cls, name: str) -&gt; Optional[Type[BaseReaderConfig]]\n</code></pre> <p>Retrieve the configuration class of a reader by its name.</p> <p></p>"},{"location":"code_reference/data/#register_reader","title":"register_reader","text":"<pre><code>@classmethod\ndef register_reader(cls,\n                    name: str,\n                    reader_cls: Type['BaseReader'],\n                    config_cls: Optional[Type[BaseReaderConfig]] = None)\n</code></pre> <p>Register reader's subclass along with its default configuration and config class.</p> <p></p>"},{"location":"code_reference/data/#read","title":"read","text":"<pre><code>@abstractmethod\ndef read(path: str) -&gt; Iterator[List[InputData]]\n</code></pre> <p>Read data from the given file path and return an iterator of lists containing InputData.</p> <p>This method is designed to read data in chunks for efficient parallel processing. The chunk size is determined by the reader's configuration.</p> <p>Arguments:</p> <ul> <li><code>path</code> str - The path to the file containing data to be read.</li> </ul> <p>Returns:</p> <ul> <li><code>Iterator[List[InputData]]</code> - An iterator yielding lists of InputData   objects.</li> </ul> <p></p>"},{"location":"code_reference/data/#generate_example_id","title":"generate_example_id","text":"<pre><code>def generate_example_id(row_data: Dict[str, Any], path: str) -&gt; str\n</code></pre> <p>Default function to generate an example_id for a given row of data.</p> <p></p>"},{"location":"code_reference/data/#yivaldatahuggingface_dataset_reader","title":"yival.data.huggingface_dataset_reader","text":""},{"location":"code_reference/dataset/","title":"Dataset","text":""},{"location":"code_reference/dataset/#yivaldatasetdata_utils","title":"yival.dataset.data_utils","text":""},{"location":"code_reference/dataset/#read_code_from_path_or_module","title":"read_code_from_path_or_module","text":"<pre><code>def read_code_from_path_or_module(path_or_module: str) -&gt; Optional[str]\n</code></pre> <p>Reads the source code either from an absolute file path or from a module, refined version.</p> <p>Arguments:</p> <ul> <li>path_or_module (str): Either an absolute path to a Python file or a module name followed by a function name.</li> </ul> <p>Returns:</p> <ul> <li>Optional[str]: The source code if found, otherwise None.</li> </ul>"},{"location":"code_reference/evaluators/","title":"Evaluators","text":""},{"location":"code_reference/evaluators/#yivalevaluatorsstring_expected_result_evaluator","title":"yival.evaluators.string_expected_result_evaluator","text":"<p>Module: string_expected_result_evaluator.py</p> <p>This module defines the StringExpectedResultEvaluator class, which is used for evaluating string expected results.</p> <p>Classes:     StringExpectedResultEvaluator: Class for evaluating string expected     results.</p> <p></p>"},{"location":"code_reference/evaluators/#is_valid_json","title":"is_valid_json","text":"<pre><code>def is_valid_json(s: str) -&gt; bool\n</code></pre> <p>Check if the given string is a valid JSON.</p> <p>Arguments:</p> <ul> <li><code>s</code> str - The input string to check.</li> </ul> <p>Returns:</p> <ul> <li><code>bool</code> - True if the input string is a valid JSON, False otherwise.</li> </ul> <p></p>"},{"location":"code_reference/evaluators/#stringexpectedresultevaluator-objects","title":"StringExpectedResultEvaluator Objects","text":"<pre><code>class StringExpectedResultEvaluator(BaseEvaluator)\n</code></pre> <p>Class for evaluating string expected results.</p> <p>This class extends the BaseEvaluator and provides specific implementation for evaluating string expected results using different matching techniques.</p> <p>Attributes:</p> <ul> <li><code>config</code> ExpectedResultEvaluatorConfig - Configuration object for the   evaluator.</li> </ul> <p></p>"},{"location":"code_reference/evaluators/#__init__","title":"__init__","text":"<pre><code>def __init__(config: ExpectedResultEvaluatorConfig)\n</code></pre> <p>Initialize the StringExpectedResultEvaluator with the provided configuration.</p> <p>Arguments:</p> <ul> <li><code>config</code> ExpectedResultEvaluatorConfig - Configuration object for   the evaluator.</li> </ul> <p></p>"},{"location":"code_reference/evaluators/#evaluate","title":"evaluate","text":"<pre><code>def evaluate(experiment_result: ExperimentResult) -&gt; EvaluatorOutput\n</code></pre> <p>Evaluate the expected result against the actual result using the specified matching technique.</p> <p>Returns:</p> <ul> <li><code>EvaluatorOutput</code> - An EvaluatorOutput object containing the   evaluation result.</li> </ul> <p></p>"},{"location":"code_reference/evaluators/#yivalevaluatorsalpaca_eval_evaluator","title":"yival.evaluators.alpaca_eval_evaluator","text":""},{"location":"code_reference/evaluators/#yivalevaluatorspython_validation_evaluator","title":"yival.evaluators.python_validation_evaluator","text":"<p>Python Validation Evaluator Module.</p> <p>This module provides an implementation of the PythonValidationEvaluator, which evaluates the raw output of an experiment using Python's exec function. The evaluator is designed to validate Python code snippets and determine whether they can be executed without any errors.</p> <p>Classes:     - PythonValidationEvaluator: Evaluates the raw output of an experiment.</p> <p></p>"},{"location":"code_reference/evaluators/#pythonvalidationevaluator-objects","title":"PythonValidationEvaluator Objects","text":"<pre><code>class PythonValidationEvaluator(BaseEvaluator)\n</code></pre> <p>Python Validation Evaluator.</p> <p>Evaluates the raw output of an experiment by attempting to execute it as Python code. If the code executes without any errors, a positive result is returned. Otherwise, a negative result is returned.</p> <p></p>"},{"location":"code_reference/evaluators/#yivalevaluatorsbase_evaluator","title":"yival.evaluators.base_evaluator","text":"<p>Evaluators Module.</p> <p>This module contains the base class and common methods for evaluators used in experiments. Evaluators are essential components in the system that interpret the results of experiments and provide quantitative or qualitative feedback. Specific evaluators are expected to inherit from the base class and implement custom evaluation logic as needed.</p> <p></p>"},{"location":"code_reference/evaluators/#baseevaluator-objects","title":"BaseEvaluator Objects","text":"<pre><code>class BaseEvaluator(ABC)\n</code></pre> <p>Base class for all evaluators.</p> <p>This class provides the basic structure and methods for evaluators. Specific evaluators should inherit from this class and implement the necessary methods.</p> <p></p>"},{"location":"code_reference/evaluators/#__init___1","title":"__init__","text":"<pre><code>def __init__(config: BaseEvaluatorConfig)\n</code></pre> <p>Initialize the evaluator with its configuration.</p> <p>Arguments:</p> <ul> <li><code>config</code> BaseEvaluatorConfig - The configuration for the evaluator.</li> </ul> <p></p>"},{"location":"code_reference/evaluators/#register","title":"register","text":"<pre><code>@classmethod\ndef register(cls, name: str)\n</code></pre> <p>Decorator to register new evaluators.</p> <p></p>"},{"location":"code_reference/evaluators/#get_evaluator","title":"get_evaluator","text":"<pre><code>@classmethod\ndef get_evaluator(cls, name: str) -&gt; Optional[Type['BaseEvaluator']]\n</code></pre> <p>Retrieve evaluator class from registry by its name.</p> <p></p>"},{"location":"code_reference/evaluators/#get_default_config","title":"get_default_config","text":"<pre><code>@classmethod\ndef get_default_config(cls, name: str) -&gt; Optional[BaseEvaluatorConfig]\n</code></pre> <p>Retrieve the default configuration of an evaluator by its name.</p> <p></p>"},{"location":"code_reference/evaluators/#get_config_class","title":"get_config_class","text":"<pre><code>@classmethod\ndef get_config_class(cls, name: str) -&gt; Optional[Type[BaseEvaluatorConfig]]\n</code></pre> <p>Retrieve the configuration class of a reader by its name.</p> <p></p>"},{"location":"code_reference/evaluators/#evaluate_1","title":"evaluate","text":"<pre><code>def evaluate(experiment_result: ExperimentResult) -&gt; EvaluatorOutput\n</code></pre> <p>Evaluate the experiment result and produce an evaluator output.</p> <p>Arguments:</p> <ul> <li><code>experiment_result</code> ExperimentResult - The result of an experiment   to be evaluated.</li> </ul> <p>Returns:</p> <ul> <li><code>EvaluatorOutput</code> - The result of the evaluation.</li> </ul> <p></p>"},{"location":"code_reference/evaluators/#aevaluate","title":"aevaluate","text":"<pre><code>async def aevaluate(experiment_result: ExperimentResult) -&gt; Any\n</code></pre> <p>Evaluate the experiment result and produce an evaluator output.</p> <p>Arguments:</p> <ul> <li><code>experiment_result</code> ExperimentResult - The result of an experiment   to be evaluated.</li> </ul> <p>Returns:</p> <ul> <li><code>EvaluatorOutput</code> - The result of the evaluation.</li> </ul> <p></p>"},{"location":"code_reference/evaluators/#evaluate_comparison","title":"evaluate_comparison","text":"<pre><code>def evaluate_comparison(group_data: List[ExperimentResult]) -&gt; None\n</code></pre> <p>Evaluate and compare a list of experiment results.</p> <p>This method is designed to evaluate multiple experiment results together, allowing for comparisons and potentially identifying trends, anomalies, or other patterns in the set of results.</p> <p>Arguments:</p> <ul> <li><code>group_data</code> List[ExperimentResult] - A list of experiment results   to be evaluated together.</li> </ul> <p>Notes:</p> <p>Implementations of this method in subclasses should handle the   specifics of how multiple experiments are evaluated and compared.</p> <p></p>"},{"location":"code_reference/evaluators/#evaluate_based_on_all_results","title":"evaluate_based_on_all_results","text":"<pre><code>def evaluate_based_on_all_results(experiment: List[Experiment]) -&gt; None\n</code></pre> <p>Evaluate based on the entirety of experiment results.</p> <p>This method evaluates an entire list of experiments, potentially taking into account all available data to produce a comprehensive evaluation.</p> <p>Arguments:</p> <ul> <li><code>experiment</code> List[Experiment] - A list of all experiments to be   evaluated.</li> </ul> <p>Notes:</p> <p>Implementations of this method in subclasses should determine how   to best utilize all available experiment data for evaluation.</p> <p></p>"},{"location":"code_reference/evaluators/#yivalevaluatorsutils","title":"yival.evaluators.utils","text":""},{"location":"code_reference/evaluators/#fuzzy_match_util","title":"fuzzy_match_util","text":"<pre><code>def fuzzy_match_util(generated: str,\n                     expected: str,\n                     threshold: int = 80) -&gt; bool\n</code></pre> <p>Matches the generated string with the expected answer(s) using fuzzy matching.</p> <p>Arguments:</p> <ul> <li><code>generated</code> str - The generated string.</li> <li><code>expected</code> str - The expected answer(s). Can be a string or list of   strings.</li> <li><code>threshold</code> int, optional - The threshold for fuzzy matching. Defaults   to 80.</li> </ul> <p>Returns:</p> <ul> <li><code>int</code> - Returns 1 if there's a match, 0 otherwise.</li> </ul> <p></p>"},{"location":"code_reference/evaluators/#yivalevaluatorsopenai_elo_evaluator","title":"yival.evaluators.openai_elo_evaluator","text":"<p>Elo Evaluators Module.</p> <p>This module contains the OpenAIEloEvaluator class, which implements an ELO-based evaluation system. The ELO system is used to rank different model outputs based on human evaluations, and this specific  implementation interfaces with the OpenAI API for those evaluations.</p> <p></p>"},{"location":"code_reference/evaluators/#k","title":"K","text":"<p>Elo rating constant</p> <p></p>"},{"location":"code_reference/evaluators/#openaieloevaluator-objects","title":"OpenAIEloEvaluator Objects","text":"<pre><code>class OpenAIEloEvaluator(BaseEvaluator)\n</code></pre> <p>OpenAIEloEvaluator is an evaluator that uses the ELO rating system to rank model outputs.</p> <p></p>"},{"location":"code_reference/evaluators/#expected_score","title":"expected_score","text":"<pre><code>def expected_score(r1, r2)\n</code></pre> <p>Calculate the expected score between two ratings.</p> <p></p>"},{"location":"code_reference/evaluators/#yivalevaluatorsrouge_evaluator","title":"yival.evaluators.rouge_evaluator","text":"<p>The Rouge Evaluator is an advanced AI tool designed to assess the quality of dialogue models.  It uses a unique approach to evaluate the responses generated by these models, focusing on aspects  such as relevance, coherence, and fluency.</p> <p>This tool is particularly useful for developers and researchers working on dialogue systems, as it allows them to measure the effectiveness of their models and make necessary enhancements. </p> <p>The Rouge Evaluator is a valuable asset for anyone looking to enhance the quality  and performance of their dialogue models.</p> <p></p>"},{"location":"code_reference/evaluators/#rougeevaluator-objects","title":"RougeEvaluator Objects","text":"<pre><code>class RougeEvaluator(BaseEvaluator)\n</code></pre> <p>Evaluator using rouge to calculate rouge score</p> <p></p>"},{"location":"code_reference/evaluators/#evaluate_2","title":"evaluate","text":"<pre><code>def evaluate(experiment_result: ExperimentResult) -&gt; EvaluatorOutput\n</code></pre> <p>Evaluate the experiment result using rouge evaluat</p> <p></p>"},{"location":"code_reference/evaluators/#main","title":"main","text":"<pre><code>def main()\n</code></pre> <p>Main function to test the rouge evaluator</p> <p></p>"},{"location":"code_reference/evaluators/#yivalevaluatorsbertscore_evaluator","title":"yival.evaluators.bertscore_evaluator","text":"<p>BERTScore is a language model evaluation metric based on the BERT language model. It leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences by cosine similarity.  It has been shown to correlate with human judgment on sentence-level and system-level evaluation.</p> <p></p>"},{"location":"code_reference/evaluators/#bertscoreevaluator-objects","title":"BertScoreEvaluator Objects","text":"<pre><code>class BertScoreEvaluator(BaseEvaluator)\n</code></pre> <p>Evaluator calculate bert_score</p> <p></p>"},{"location":"code_reference/evaluators/#evaluate_3","title":"evaluate","text":"<pre><code>def evaluate(experiment_result: ExperimentResult) -&gt; EvaluatorOutput\n</code></pre> <p>Evaluate the experiment result according to bertsocre</p> <p></p>"},{"location":"code_reference/evaluators/#main_1","title":"main","text":"<pre><code>def main()\n</code></pre> <p>Main function to test the bertscore evaluator</p> <p></p>"},{"location":"code_reference/evaluators/#yivalevaluatorsopenai_prompt_based_evaluator","title":"yival.evaluators.openai_prompt_based_evaluator","text":"<p>OpenAIPromptBasedEvaluator is an evaluator that uses OpenAI's prompt-based system for evaluations.</p> <p>The evaluator interfaces with the OpenAI API to present tasks and interpret the model's responses to determine the quality or correctness of a given experiment result.</p> <p></p>"},{"location":"code_reference/evaluators/#extract_choice_from_response","title":"extract_choice_from_response","text":"<pre><code>def extract_choice_from_response(response: str,\n                                 choice_strings: Iterable[str]) -&gt; str\n</code></pre> <p>Extracts the choice from the response string.</p> <p></p>"},{"location":"code_reference/evaluators/#calculate_choice_score","title":"calculate_choice_score","text":"<pre><code>def calculate_choice_score(\n        choice: str,\n        choice_scores: Optional[Dict[str, float]] = None) -&gt; Optional[float]\n</code></pre> <p>Calculates the score for the given choice.</p> <p></p>"},{"location":"code_reference/evaluators/#format_template","title":"format_template","text":"<pre><code>def format_template(\n        template: Union[str, List[Dict[str, str]]],\n        content: Dict[str, Any]) -&gt; Union[str, List[Dict[str, str]]]\n</code></pre> <p>Formats a string or list template with the provided content.</p> <p></p>"},{"location":"code_reference/evaluators/#choices_to_string","title":"choices_to_string","text":"<pre><code>def choices_to_string(choice_strings: Iterable[str]) -&gt; str\n</code></pre> <p>Converts a list of choices into a formatted string.</p> <p></p>"},{"location":"code_reference/evaluators/#openaipromptbasedevaluator-objects","title":"OpenAIPromptBasedEvaluator Objects","text":"<pre><code>class OpenAIPromptBasedEvaluator(BaseEvaluator)\n</code></pre> <p>Evaluator using OpenAI's prompt-based evaluation.</p> <p></p>"},{"location":"code_reference/evaluators/#evaluate_4","title":"evaluate","text":"<pre><code>def evaluate(experiment_result: ExperimentResult) -&gt; EvaluatorOutput\n</code></pre> <p>Evaluate the experiment result using OpenAI's prompt-based evaluation.</p> <p></p>"},{"location":"code_reference/evaluators/#main_2","title":"main","text":"<pre><code>def main()\n</code></pre> <p>Main function to test the OpenAIPromptBasedEvaluator.</p>"},{"location":"code_reference/experiment/","title":"Experiment","text":""},{"location":"code_reference/experiment/#yivalexperimentdata_processor","title":"yival.experiment.data_processor","text":""},{"location":"code_reference/experiment/#dataprocessor-objects","title":"DataProcessor Objects","text":"<pre><code>class DataProcessor()\n</code></pre> <p>Utility class to process data based on DatasetConfig.</p> <p></p>"},{"location":"code_reference/experiment/#process_data","title":"process_data","text":"<pre><code>def process_data() -&gt; Iterator[List[InputData]]\n</code></pre> <p>Processes data based on the DatasetConfig and returns the processed data.</p> <p></p>"},{"location":"code_reference/experiment/#yivalexperimentexperiment_runner","title":"yival.experiment.experiment_runner","text":""},{"location":"code_reference/experiment/#experimentrunner-objects","title":"ExperimentRunner Objects","text":"<pre><code>class ExperimentRunner()\n</code></pre>"},{"location":"code_reference/experiment/#parallel_task","title":"parallel_task","text":"<pre><code>def parallel_task(data_point, all_combinations, logger, evaluator)\n</code></pre> <p>Task to be run in parallel for processing data points.</p> <p></p>"},{"location":"code_reference/experiment/#run","title":"run","text":"<pre><code>def run(display: bool = True,\n        interactive: bool = False,\n        output_path: Optional[str] = \"abc.pkl\",\n        experiment_input_path: Optional[str] = \"abc.pkl\",\n        async_eval: bool = False,\n        enhance_page: bool = False)\n</code></pre> <p>Run the experiment based on the source type and provided configuration.</p> <p></p>"},{"location":"code_reference/experiment/#yivalexperimentappapp","title":"yival.experiment.app.app","text":""},{"location":"code_reference/experiment/#include_image_base64","title":"include_image_base64","text":"<pre><code>def include_image_base64(data_dict)\n</code></pre> <p>Check if a string includes a base64 encoded image.</p> <p></p>"},{"location":"code_reference/experiment/#include_video","title":"include_video","text":"<pre><code>def include_video(data_dict)\n</code></pre> <p>Check if a string includes a video.</p> <p></p>"},{"location":"code_reference/experiment/#is_base64_image","title":"is_base64_image","text":"<pre><code>def is_base64_image(value)\n</code></pre> <p>Check if a string is a base64 encoded image.</p> <p></p>"},{"location":"code_reference/experiment/#base64_to_img","title":"base64_to_img","text":"<pre><code>def base64_to_img(base64_string)\n</code></pre> <p>Convert a base64 string into a PIL Image.</p> <p></p>"},{"location":"code_reference/experiment/#extract_and_decode_image_from_string","title":"extract_and_decode_image_from_string","text":"<pre><code>def extract_and_decode_image_from_string(data_string)\n</code></pre> <p>Extract and decode the first image from a string include base64 encoded and return a dictionary.</p> <p></p>"},{"location":"code_reference/experiment/#extract_and_decode_video_from_string","title":"extract_and_decode_video_from_string","text":"<pre><code>def extract_and_decode_video_from_string(data_string)\n</code></pre> <p>Extract and decode the first video from a string include video urls and return a dictionary.</p> <p></p>"},{"location":"code_reference/experiment/#extract_and_decode_image","title":"extract_and_decode_image","text":"<pre><code>def extract_and_decode_image(data_dict)\n</code></pre> <p>Extract and decode image from a dictionary include base64 encoded.</p> <p></p>"},{"location":"code_reference/experiment/#extract_and_decode_video","title":"extract_and_decode_video","text":"<pre><code>def extract_and_decode_video(data_dict)\n</code></pre> <p>Extract and decode video from a dictionary include video url.</p> <p></p>"},{"location":"code_reference/experiment/#create_table","title":"create_table","text":"<pre><code>def create_table(data)\n</code></pre> <p>Create an HTML table from a list of dictionaries, where the values can be strings or PIL images.</p> <p></p>"},{"location":"code_reference/experiment/#create_video_table","title":"create_video_table","text":"<pre><code>def create_video_table(data)\n</code></pre> <p>Create an HTML table from a list of dictionaries, where the values can be strings or video.</p> <p></p>"},{"location":"code_reference/experiment/#yivalexperimentapputils","title":"yival.experiment.app.utils","text":""},{"location":"code_reference/experiment/#image_to_base64","title":"image_to_base64","text":"<pre><code>def image_to_base64(image: Image.Image) -&gt; str\n</code></pre> <p>Converts an image to base64 string.</p> <p></p>"},{"location":"code_reference/experiment/#generate_legend","title":"generate_legend","text":"<pre><code>def generate_legend()\n</code></pre> <p>Generates the legend for the heatmap.</p> <p></p>"},{"location":"code_reference/experiment/#yivalexperimentapp","title":"yival.experiment.app","text":""},{"location":"code_reference/experiment/#yivalexperimentapphexagram","title":"yival.experiment.app.hexagram","text":""},{"location":"code_reference/experiment/#yivalexperimentstable_diffusion","title":"yival.experiment.stable_diffusion","text":""},{"location":"code_reference/experiment/#yivalexperimentbotinteractive_bot","title":"yival.experiment.bot.interactive_bot","text":""},{"location":"code_reference/experiment/#yivalexperimentbot","title":"yival.experiment.bot","text":""},{"location":"code_reference/experiment/#yivalexperimentbotrun_streamlit","title":"yival.experiment.bot.run_streamlit","text":""},{"location":"code_reference/experiment/#extract_params","title":"extract_params","text":"<pre><code>def extract_params(input_str)\n</code></pre> <p>Extract the parameters from the user input.</p> <p></p>"},{"location":"code_reference/experiment/#display_image","title":"display_image","text":"<pre><code>def display_image(image_list)\n</code></pre> <p>Display the image output from the experiment.</p> <p></p>"},{"location":"code_reference/experiment/#run_experiments","title":"run_experiments","text":"<pre><code>def run_experiments(selected_combinations, input_data, experiment_config,\n                    logger, evaluator)\n</code></pre> <p>Run the experiment with lite_experiment</p> <p></p>"},{"location":"code_reference/experiment/#display_results","title":"display_results","text":"<pre><code>def display_results(results)\n</code></pre> <p>Display the results with bot messages after the experiment.</p> <p></p>"},{"location":"code_reference/experiment/#run_streamlit","title":"run_streamlit","text":"<pre><code>def run_streamlit()\n</code></pre> <p>Run the experiment using the user input and pkl file.</p> <p></p>"},{"location":"code_reference/experiment/#yivalexperimentevaluator","title":"yival.experiment.evaluator","text":""},{"location":"code_reference/experiment/#evaluator-objects","title":"Evaluator Objects","text":"<pre><code>class Evaluator()\n</code></pre> <p>Utility class to evaluate ExperimentResult.</p> <p></p>"},{"location":"code_reference/experiment/#yivalexperimentutils","title":"yival.experiment.utils","text":""},{"location":"code_reference/experiment/#import_function_from_string","title":"import_function_from_string","text":"<pre><code>def import_function_from_string(func_string: str)\n</code></pre> <p>Helper function to import a function from a string.</p> <p></p>"},{"location":"code_reference/experiment/#get_yaml_args","title":"get_yaml_args","text":"<pre><code>def get_yaml_args(dataset)\n</code></pre> <p>Get argument from yaml.</p> <p></p>"},{"location":"code_reference/experiment/#get_function_args","title":"get_function_args","text":"<pre><code>def get_function_args(func_string: str, dataset: dict)\n</code></pre> <p>Get argument types of a function.</p> <p></p>"},{"location":"code_reference/experiment/#call_function_from_string","title":"call_function_from_string","text":"<pre><code>def call_function_from_string(func_string: str, **kwargs) -&gt; Any\n</code></pre> <p>Call a function specified by a string.</p> <p></p>"},{"location":"code_reference/experiment/#yivalexperimentlite_experiment","title":"yival.experiment.lite_experiment","text":""},{"location":"code_reference/experiment/#liteexperimentrunner-objects","title":"LiteExperimentRunner Objects","text":"<pre><code>class LiteExperimentRunner()\n</code></pre> <p>LiteExperimentRunner  </p> <p>This runner is designed for situation you already have assemble all datas &amp; evaluators... And just want to get experiment results on given data</p> <p>And you can easily update all variations through /update_variations()/  </p> <p>:)</p> <p></p>"},{"location":"code_reference/experiment/#parallel_task_1","title":"parallel_task","text":"<pre><code>def parallel_task(data, all_combinations, logger, evaluator)\n</code></pre> <p>Execute a single input run in parallel</p> <p></p>"},{"location":"code_reference/experiment/#set_variations","title":"set_variations","text":"<pre><code>def set_variations(variations: List[Dict[str, List[str]]])\n</code></pre> <p>set all variations for current experiment variations are format in  [     {         var1_name: [\"a\",\"b\",\"c\"]     },     {         var2_name: [\"d\",\"e\",\"f\"]     } ]</p> <p></p>"},{"location":"code_reference/experiment/#yivalexperimentrate_limiter","title":"yival.experiment.rate_limiter","text":""},{"location":"code_reference/finetune/","title":"Finetune","text":""},{"location":"code_reference/finetune/#yivalfinetuneback_up_trainer","title":"yival.finetune.back_up_trainer","text":"<p>This module is the back up trainer</p> <p>It will only be called when the dependency is not imported correctly</p> <p></p>"},{"location":"code_reference/finetune/#yivalfinetunesft_trainer","title":"yival.finetune.sft_trainer","text":"<p>This module provides an implementation of Supervised Fine-tuning trainer.</p> <p></p>"},{"location":"code_reference/finetune/#sfttrainer-objects","title":"SFTTrainer Objects","text":"<pre><code>class SFTTrainer(BaseTrainer)\n</code></pre> <p>SFT Trainer implement</p> <p></p>"},{"location":"code_reference/finetune/#yivalfinetuneopenai_finetune_utils","title":"yival.finetune.openai_finetune_utils","text":""},{"location":"code_reference/finetune/#finetune","title":"finetune","text":"<pre><code>def finetune(input_file: str,\n             condition: str,\n             custom_function: str,\n             system_prompt: str = \"\",\n             model_suffx: str = \"\") -&gt; str\n</code></pre> <p>Fine-tunes a gpt-3.5 using provided data and conditions.</p> <p>Arguments:</p> <ul> <li>input_file (str): Path to the input file containing experiment results.</li> <li>condition (str): Condition to evaluate for extracting relevant results.</li> <li>custom_function (str): Path or module containing the custom function used in the experiment.</li> <li>system_prompt (str, optional): System message to prepend to each chat. Defaults to None.</li> <li>model_suffix: (str, optional): Suffix to append to the model name. Defaults to None.</li> </ul> <p>Returns:</p> <ul> <li>str: ID of the fine-tuned model.</li> </ul> <p></p>"},{"location":"code_reference/finetune/#yivalfinetunebase_trainer","title":"yival.finetune.base_trainer","text":"<p>This module defines the base class for trainer</p> <p>Trainers are responsible for finetune llms locally based on  the data and experiment results</p> <p></p>"},{"location":"code_reference/finetune/#basetrainer-objects","title":"BaseTrainer Objects","text":"<pre><code>class BaseTrainer(ABC)\n</code></pre> <p>Abstract base class for all trainers</p> <p>Attributes:</p> <ul> <li><code>_register</code> Dict[str, Dict[str,Any]] - A register to keep track of   trainers</li> <li><code>default_config</code> Optional[BaseTrainerConfig] - Default configuration   for the trainers</li> </ul> <p></p>"},{"location":"code_reference/finetune/#get_trainer","title":"get_trainer","text":"<pre><code>@classmethod\ndef get_trainer(cls, name: str) -&gt; Optional[Type['BaseTrainer']]\n</code></pre> <p>Retrieve trainer class from registry by its name.</p> <p></p>"},{"location":"code_reference/finetune/#get_default_config","title":"get_default_config","text":"<pre><code>@classmethod\ndef get_default_config(cls, name: str) -&gt; Optional[BaseTrainerConfig]\n</code></pre> <p>Retrieve the default configuration of a trainer from the name</p> <p></p>"},{"location":"code_reference/finetune/#register_trainer","title":"register_trainer","text":"<pre><code>@classmethod\ndef register_trainer(cls,\n                     name: str,\n                     trainer_cls: Type['BaseTrainer'],\n                     config_cls: Optional[Type[BaseTrainerConfig]] = None)\n</code></pre> <p>Register a new trainer along with its defualt configuration and configuration class.</p> <p></p>"},{"location":"code_reference/finetune/#train","title":"train","text":"<pre><code>@abstractmethod\ndef train(experiment: Experiment, config: ExperimentConfig) -&gt; TrainerOutput\n</code></pre> <p>Train models based on the configs and datas</p> <p>Arguments:</p> <ul> <li><code>experiment</code> Experiment - The experiment with its results.</li> <li><code>config</code> ExperimentConfig - The original experiment configuration.</li> <li><code>evaluator</code> Evaluator - A utility class to evaluate the   ExperimentResult. token_logger (TokenLogger): Logs the token usage.</li> </ul> <p>Returns:</p> <p>TrainerOutput</p> <p></p>"},{"location":"code_reference/finetune/#yivalfinetuneutils","title":"yival.finetune.utils","text":""},{"location":"code_reference/finetune/#print_trainable_parameters","title":"print_trainable_parameters","text":"<pre><code>def print_trainable_parameters(model)\n</code></pre> <p>Prints the number of trainable parameters in the model.</p> <p></p>"},{"location":"code_reference/finetune/#extract_from_input_data","title":"extract_from_input_data","text":"<pre><code>def extract_from_input_data(experiment: Experiment, prompt_key: str,\n                            completion_key: str | None,\n                            condition: str | None) -&gt; HgDataset\n</code></pre> <p>if experiment doesn't support custom_func , extract all data from group_experiment_results</p> <p>else extract data from combination_aggregated_metrics according to condition</p> <p>An example of condition: 'name == openai_prompt_based_evaluator AND result &gt;= 0 AND display_name == clarity'</p> <p></p>"},{"location":"code_reference/finetune/#yivalfinetunereplicate_finetune_utils","title":"yival.finetune.replicate_finetune_utils","text":""},{"location":"code_reference/finetune/#finetune_1","title":"finetune","text":"<pre><code>def finetune(input_file: str,\n             condition: str,\n             custom_function: str,\n             destination: str,\n             model_name: str,\n             num_train_epochs: int = 3,\n             support_expected_value: bool = False,\n             system_prompt: str = \"\") -&gt; str\n</code></pre> <p>Fine-tunes a replicate model using provided data and conditions.</p> <p>Arguments:</p> <ul> <li>input_file (str): Path to the input file containing experiment results.</li> <li>condition (str): Condition to evaluate for extracting relevant results.</li> <li>custom_function (str): Path or module containing the custom function used in the experiment.</li> <li>destination: (str): The model to push the trained version to .</li> <li>model_name: (str): Model name that will be used for finetune.</li> <li>num_train_epochs: (int, optional): Number of epochs to train the model.</li> <li>system_prompt (str, optional): System message to prepend to each chat. Defaults to None.</li> </ul> <p>Returns:</p> <ul> <li>str: ID of the fine-tuned model.</li> </ul>"},{"location":"code_reference/output-parsers/","title":"Output Parsers","text":""},{"location":"code_reference/output-parsers/#yivaloutput_parsersbase_parser","title":"yival.output_parsers.base_parser","text":""},{"location":"code_reference/output-parsers/#baseparserwithregistry-objects","title":"BaseParserWithRegistry Objects","text":"<pre><code>class BaseParserWithRegistry()\n</code></pre> <p>Base class for parsers that provides automatic registration of subclasses. Any subclass that inherits from this base class will be automatically added to the registry. The registry can then be used to retrieve a parser class based on its name.</p> <p></p>"},{"location":"code_reference/output-parsers/#registry","title":"registry","text":"<p>Class-level registry for all parser subclasses</p> <p></p>"},{"location":"code_reference/output-parsers/#__init_subclass__","title":"__init_subclass__","text":"<pre><code>def __init_subclass__(cls, **kwargs)\n</code></pre> <p>Automatically called when a subclass is defined.</p> <p></p>"},{"location":"code_reference/output-parsers/#parse","title":"parse","text":"<pre><code>def parse(output: str) -&gt; List[str]\n</code></pre> <p>Parse the provided output. This method should be overridden by subclasses to provide custom parsing logic.</p> <p></p>"},{"location":"code_reference/output-parsers/#yivaloutput_parsersutils","title":"yival.output_parsers.utils","text":""},{"location":"code_reference/output-parsers/#capture_and_parse_with_base_registry","title":"capture_and_parse_with_base_registry","text":"<pre><code>def capture_and_parse_with_base_registry(config=None)\n</code></pre> <p>Decorator to capture stdout of a function and parse it using a specified parser.</p> <p>The parser is determined based on the provided configuration. If the specified parser is not found in the BaseParserWithRegistry's registry, the function's output will not be captured or parsed.</p> <p>Arguments:</p> <ul> <li>config (dict, optional): Configuration dict with 'parser' key specifying   the parser class name.</li> </ul> <p>Returns:</p> <ul> <li>Decorated function that captures and parses its stdout.</li> </ul>"},{"location":"code_reference/result-selectors/","title":"Result Selectors","text":""},{"location":"code_reference/result-selectors/#yivalresult_selectorsselection_context","title":"yival.result_selectors.selection_context","text":""},{"location":"code_reference/result-selectors/#yivalresult_selectorsahp_selection","title":"yival.result_selectors.ahp_selection","text":""},{"location":"code_reference/result-selectors/#yivalresult_selectorsnormalize_func","title":"yival.result_selectors.normalize_func","text":"<p>Normalization functions</p> <p></p>"},{"location":"code_reference/result-selectors/#min_max_normalization","title":"min_max_normalization","text":"<pre><code>def min_max_normalization(matrix: np.ndarray) -&gt; np.ndarray\n</code></pre> <p>normalize matrix in min_max method</p> <p></p>"},{"location":"code_reference/result-selectors/#z_score_normalizatioin","title":"z_score_normalizatioin","text":"<pre><code>def z_score_normalizatioin(matrix: np.ndarray) -&gt; np.ndarray\n</code></pre> <p>normalize matrix in z_score method</p> <p></p>"},{"location":"code_reference/result-selectors/#yivalresult_selectorsselection_strategy","title":"yival.result_selectors.selection_strategy","text":"<p>Selection Strategy Module.</p> <p>This module defines an abstract base class for selection strategies. A selection strategy  determines how to select or prioritize specific experiments, scenarios, or configurations based on certain criteria.</p> <p></p>"},{"location":"code_reference/result-selectors/#selectionstrategy-objects","title":"SelectionStrategy Objects","text":"<pre><code>class SelectionStrategy(ABC)\n</code></pre> <p>Abstract base class for selection strategies.</p> <p></p>"},{"location":"code_reference/result-selectors/#get_strategy","title":"get_strategy","text":"<pre><code>@classmethod\ndef get_strategy(cls, name: str) -&gt; Optional[Type['SelectionStrategy']]\n</code></pre> <p>Retrieve strategy class from registry by its name.</p> <p></p>"},{"location":"code_reference/result-selectors/#get_default_config","title":"get_default_config","text":"<pre><code>@classmethod\ndef get_default_config(cls, name: str) -&gt; Optional[BaseConfig]\n</code></pre> <p>Retrieve the default configuration of a strategy by its name.</p>"},{"location":"eng-spec/architecture/","title":"Architecture","text":"<pre><code>flowchart TD\n\n    %% Data Generation Stage\n    A[Start]\n    A --&gt; |Data Generation| A1[Dataset]\n    A1 --&gt; A2[DataGenerator]\n    A2 --&gt; A3[Specific Generator]\n    A1 --&gt; A4[DataReader]\n    A4 --&gt; A5[Specific Reader]\n    A1 --&gt; A6[Manual Input]\n\n    %% Create Combinations Stage\n    A1 --&gt; |Create Combinations| B\n    B --&gt; B1[Specific Combination Creator]\n    B --&gt; B2[Set Combinations Manually]\n    B --&gt; B3[List of Combinations]\n\n    %% Evaluate Stage\n    B3 --&gt; |Analysis| C\n    C --&gt; C1[User's Function]\n    C --&gt; C2[Results from Function]\n    A1 --&gt; C1\n\n    %% Evaluator Stage\n    C2 --&gt; |Evaluation| D\n    D --&gt; D1[Method 1]\n    D --&gt; D2[Method 2]\n    D --&gt; D3[Method 3]\n\n    %% Select Stage\n    D --&gt; |Selection| E\n\n    %% Enhancer Stage\n    E --&gt; |Enhancement| F\n    F --&gt; F1[Enhancer]\n    F1 --&gt; C\n\n    %% Trainer Stage\n    E --&gt; |Finetune| G\n    A1 --&gt; |Finetune| G\n    G --&gt; G1[Trainer]\n\n\n    %% Styling\n    style A fill:#f9d77e,stroke:#f96e5b\n    style B fill:#a1d4c6,stroke:#f96e5b\n    style C fill:#f6c3d5,stroke:#f96e5b\n    style D fill:#b2b1cf,stroke:#f96e5b\n    style E fill:#f9efaa,stroke:#f96e5b\n    style F fill:#f2a3b3,stroke:#f96e5b\n    style G fill:#a3a3a3,stroke:#f96e5b\n</code></pre>"},{"location":"eng-spec/architecture/#data-generation","title":"Data Generation","text":"<p>The process starts with the generation of a dataset which can come from multiple sources:</p> <ul> <li>Specific Data Generator: A defined method or algorithm that automatically                                churns out data.</li> <li>Data Reader: A component that reads data from external places.</li> <li>Manual Input: As straightforward as it sounds, data can be added manually.</li> </ul>"},{"location":"eng-spec/architecture/#combination-creation","title":"Combination Creation","text":"<p>Once we have our dataset, we form combinations that are pivotal for the subsequent analysis:</p> <ul> <li>Formed using specific combination creators.</li> <li>Defined manually.</li> </ul>"},{"location":"eng-spec/architecture/#analysis","title":"Analysis","text":"<p>This is the heart of the Yival framework. A custom function provided by the user takes in the dataset and combination list to produce valuable insights.</p>"},{"location":"eng-spec/architecture/#evaluation","title":"Evaluation","text":"<p>After analysis, the results are subjected to evaluation. Several methodologies can be applied to grasp and gauge the data's behavior deeply.</p>"},{"location":"eng-spec/architecture/#selection","title":"Selection","text":"<p>From the evaluations, the most promising results are selected. This process ensures only the most vital insights are pushed forward.</p>"},{"location":"eng-spec/architecture/#enhancement","title":"Enhancement","text":"<p>The selected results are then fine-tuned in this phase. An \"Enhancer\" is applied to enhance these results. This stage can loop back to the analysis stage, indicating an ongoing, iterative process of refinement.</p>"},{"location":"eng-spec/architecture/#trainer","title":"Trainer","text":"<p>Yival also supports finetuning models, and we offer two methods:</p> <ul> <li>Use various dataset generators built into Yival (including Huggingface, etc.) for   data upload or generation, and then finetune the model.</li> <li>Provide a custom_func, use advanced models like GPT-4 for data generation, and    customize the selection criteria. The model is then finetuned based on the  selected data.</li> </ul> <p>Yival supports all base models supported by Huggingface and provides advanced finetuning methods such as LoRA, 8bit/4bit quantization.</p>"},{"location":"eng-spec/custom_parent/custom-combination-enhancer/","title":"Custom Combination Enhancer Creation Guide","text":""},{"location":"eng-spec/custom_parent/custom-combination-enhancer/#introduction","title":"Introduction","text":"<p>Combination enhancers play a pivotal role in the experimental framework by optimizing the combination of experiments based on their outcomes. By leveraging combination enhancers, experiments can be fine-tuned to achieve better results. This guide will outline the process of creating a custom combination enhancer.</p>"},{"location":"eng-spec/custom_parent/custom-combination-enhancer/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Overview of Base Combination Enhancer</li> <li>Implementing a Custom Combination Enhancer</li> <li>Registering the Custom Combination Enhancer</li> <li>Conclusion</li> </ol>"},{"location":"eng-spec/custom_parent/custom-combination-enhancer/#overview-of-base-combination-enhancer","title":"Overview of Base Combination Enhancer","text":"<p>The <code>BaseCombinationEnhancer</code> class provides the foundational structure for all combination enhancers. It offers methods to:</p> <ul> <li>Register new combination enhancers.</li> <li>Fetch registered combination enhancers.</li> <li>Retrieve their default configurations.</li> </ul> <p>The main responsibility of a combination enhancer is to enhance the setup of experiments based on their results.</p>"},{"location":"eng-spec/custom_parent/custom-combination-enhancer/#implementing-a-custom-combination-enhancer","title":"Implementing a Custom Combination Enhancer","text":"<p>To create a custom combination enhancer, one should inherit from the <code>BaseCombinationEnhancer</code> class and implement the <code>enhance</code> abstract method:</p> <pre><code>class CustomCombinationEnhancer(BaseCombinationEnhancer):\n    \"\"\"\n    Custom combination enhancer to optimize the setup of experiments.\n    \"\"\"\n\n    def enhance(self, experiment, config, evaluator, token_logger):\n        \"\"\"\n        Custom logic to enhance the experiment based on its results.\n\n        Args:\n            experiment (Experiment): The experiment with its results.\n            config (ExperimentConfig): The original experiment configuration.\n            evaluator (Evaluator): A utility class to evaluate the\n            ExperimentResult.\n            token_logger (TokenLogger): Logs the token usage.\n\n        Returns:\n            EnhancerOutput: The result of the enhancement.\n        \"\"\"\n\n        # Custom logic for enhancement goes here\n        pass\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-combination-enhancer/#config","title":"Config","text":"<pre><code>custom_enhancers:\n    class: /path/to/custom_enhancer.CustomEnhancer\n    config_cls: /path/to/custom_enhancer_config.CustomEnhancerConfig\n</code></pre> <p>To use it</p> <pre><code>enhancer:\n  name: custom_enhancer\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-combination-enhancer/#conclusion","title":"Conclusion","text":"<p>By following this guide, you have successfully created and registered a custom combination enhancer named <code>CustomCombinationEnhancer</code> within the experimental framework. This custom enhancer will allow you to optimize experiment combinations based on specific logic and criteria you define. As experiments evolve and grow in complexity, custom combination enhancers like the one you've developed will become instrumental in achieving more refined and better results.</p>"},{"location":"eng-spec/custom_parent/custom-data-generator/","title":"Creating a Custom Data Generator with <code>BaseDataGenerator</code>","text":"<p>This guide will walk you through creating a custom data generator using the provided <code>BaseDataGenerator</code>.</p>"},{"location":"eng-spec/custom_parent/custom-data-generator/#introduction","title":"Introduction","text":"<p>The ability to programmatically generate data is crucial in scenarios where synthetic or mock data is required, such as in testing, simulations, and more. The provided foundational architecture for data generators allows for flexibility and extensibility, enabling you to create custom data generators tailored to specific needs.</p> <p>In this guide, we will demonstrate how to create a custom data generator by extending the BaseDataGenerator. Our custom generator will output a list of predefined strings. By following this guide, you'll gain an understanding of the structure and process, enabling you to develop even more complex generators as needed.</p>"},{"location":"eng-spec/custom_parent/custom-data-generator/#step-1-subclassing-the-basedatagenerator","title":"Step 1: Subclassing the <code>BaseDataGenerator</code>","text":"<p>First, create a <code>ListStringDataGenerator</code> that simply outputs a list of strings as specified in its configuration.</p> <pre><code>from typing import Iterator, List\n\nfrom list_string_data_generator_config import ListStringGeneratorConfig\nfrom yival.data_generators.base_data_generator import BaseDataGenerator\nfrom yival.schemas.common_structures import InputData\n\n\n\nclass ListStringDataGenerator(BaseDataGenerator):\n    def __init__(self, config: 'ListStringGeneratorConfig'):\n        super().__init__(config)\n\n    def generate_examples(self) -&gt; Iterator[List[InputData]]:\n        for string_data in self.config.strings_to_generate:\n            yield [InputData(example_id=self.generate_example_id(string_data), content=string_data)]\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-data-generator/#step-2-providing-a-configuration-class","title":"Step 2: Providing a Configuration Class","text":"<p>To specify the list of strings our generator should output, define a custom configuration class:</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import List\n\nfrom yival.schemas.data_generator_configs import BaseDataGeneratorConfig\n\n\n@dataclass\nclass ListStringGeneratorConfig(BaseDataGeneratorConfig):\n    \"\"\"\n    Configuration for the ListStringDataGenerator.\n    \"\"\"\n    strings_to_generate: List[str] = field(default_factory=list)\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-data-generator/#config","title":"Config","text":"<p>In your configuration (YAML), you can now specify the use of this data generator:</p> <pre><code>custom_data_generators:\n  list_string_data_generator:\n    class: /path/to/list_string_data_generator.ListStringDataGenerator\n    config_cls: /path/to/list_string_data_generator_config.ListStringGeneratorConfig\n</code></pre> <pre><code>dataset:\n  data_generators:\n    list_string_data_generator:\n      strings_to_generate:\n        - abc\n        - def\n  source_type: machine_generated\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-evaluator/","title":"Custom Evaluator Creation Guide: SimpleEvaluator","text":""},{"location":"eng-spec/custom_parent/custom-evaluator/#introduction","title":"Introduction","text":"<p>Evaluators are central components in the experimental framework that interpret experiment results and offer either quantitative or qualitative feedback. This guide will walk you through the steps of creating a custom evaluator, named <code>SimpleEvaluator</code>, which returns a value of 1 if the result is 1, and 0 otherwise.</p>"},{"location":"eng-spec/custom_parent/custom-evaluator/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Overview of Base Evaluator</li> <li>Creating the SimpleEvaluator Configuration</li> <li>Implementing the SimpleEvaluator</li> <li>Registering the SimpleEvaluator</li> <li>Conclusion</li> </ol>"},{"location":"eng-spec/custom_parent/custom-evaluator/#overview-of-base-evaluator","title":"Overview of Base Evaluator","text":"<p>The <code>BaseEvaluator</code> class provides the foundational structure for all evaluators. It offers methods to register new evaluators, fetch registered evaluators, and retrieve their configurations. The primary purpose of evaluators is to interpret and analyze the results of experiments based on their unique evaluation logic.</p>"},{"location":"eng-spec/custom_parent/custom-evaluator/#creating-the-simpleevaluator-configuration","title":"Creating the SimpleEvaluator Configuration","text":"<p>Before creating the evaluator, we define the configuration for our <code>SimpleEvaluator</code>. The configuration helps in defining how the evaluator should behave and what parameters it may require.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any, Dict, List\n\nfrom yival.schemas.evaluator_config import (\n    BaseEvaluatorConfig,\n    EvaluatorType,\n    MetricCalculatorConfig,\n)\n\n\n@dataclass\nclass SimpleEvaluatorConfig(BaseEvaluatorConfig):\n    \"\"\"\n    Configuration for SimpleEvaluator.\n    \"\"\"\n    metric_calculators: List[MetricCalculatorConfig] = field(\n        default_factory=list\n    )\n    evaluator_type = EvaluatorType.INDIVIDUAL\n\n    def asdict(self) -&gt; Dict[str, Any]:\n        base_dict = super().asdict()\n        base_dict[\"metric_calculators\"] = [\n            mc.asdict() for mc in self.metric_calculators\n        ]\n        return base_dict\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-evaluator/#implementing-the-simpleevaluator","title":"Implementing the SimpleEvaluator","text":"<p>Now, let's create the <code>SimpleEvaluator</code> that utilizes the above configuration:</p> <pre><code>from simple_evaluator import SimpleEvaluatorConfig\nfrom yival.evaluators.base_evaluator import BaseEvaluator\nfrom yival.schemas.evaluator_config import EvaluatorOutput\n\n\n@BaseEvaluator.register(\"simple_evaluator\")\nclass SimpleEvaluator(BaseEvaluator):\n    \"\"\"\n    A basic evaluator that returns a value of 1 if the result is 1, and 0\n    otherwise.\n    \"\"\"\n\n    def __init__(self, config: SimpleEvaluatorConfig):\n        super().__init__(config)\n\n    def evaluate(self, experiment_result) -&gt; EvaluatorOutput:\n        \"\"\"\n        Evaluate the experiment result and produce an evaluator output.\n\n        Args:\n            experiment_result: The result of an experiment to be evaluated.\n\n        Returns:\n            EvaluatorOutput: The result of the evaluation.\n        \"\"\"\n        result = 1 if experiment_result == 1 else 0\n        return EvaluatorOutput(name=\"Simple Evaluation\", result=result)\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-evaluator/#config","title":"Config","text":"<p>Next you can config the evaluator</p> <pre><code>custom_evaluators:\n  simple_evaluator:\n    class: /path/to/simple_evaluator.SimpleEvaluator\n    config_cls: /path/to/simple_evaluator_config.SimpleEvaluatorConfig\n</code></pre> <p>And use it</p> <pre><code>evaluators:\n  - name: simple_evaluator\n    simple_evaluator:\n      metric_calculators: []\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-evaluator/#conclusion","title":"Conclusion","text":"<p>By following this guide, you've successfully developed, configured, and registered  a custom evaluator named <code>SimpleEvaluator</code> within the experimental framework.  Custom evaluators, like the one you've created, enable a tailored approach to interpreting and analyzing experiment results, ensuring the specific needs of an experiment are met.</p>"},{"location":"eng-spec/custom_parent/custom-reader/","title":"Writing a Custom Data Reader in Python with BaseReader","text":"<p>This guide provides steps on how to create custom data readers by subclassing the provided <code>BaseReader</code> class. The example demonstrates how to create a <code>TXTReader</code> to read <code>.txt</code> files.</p>"},{"location":"eng-spec/custom_parent/custom-reader/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>BaseReader Overview</li> <li>Creating a Custom Reader (TXTReader)</li> <li>Conclusion</li> </ol>"},{"location":"eng-spec/custom_parent/custom-reader/#1-introduction","title":"1. Introduction","text":"<p>Data readers are responsible for reading data from various sources. By subclassing the <code>BaseReader</code>, you can create custom readers tailored to your specific data format needs.</p>"},{"location":"eng-spec/custom_parent/custom-reader/#2-basereader-overview","title":"2. BaseReader Overview","text":"<p>The <code>BaseReader</code> class offers a blueprint for designing data readers. It has methods to:</p> <ul> <li>Register new readers.</li> <li>Retrieve registered readers and their configurations.</li> <li>Read data in chunks.</li> </ul> <p>The class provides an abstract method <code>read</code> that you must override in your custom reader. The method is designed to read data in chunks for efficient parallel processing.</p>"},{"location":"eng-spec/custom_parent/custom-reader/#3-creating-a-custom-reader-txtreader","title":"3. Creating a Custom Reader (TXTReader)","text":""},{"location":"eng-spec/custom_parent/custom-reader/#31-design-the-txtreaderconfig-class","title":"3.1. Design the TXTReaderConfig Class","text":"<p>Before creating the reader, design a configuration class specific to the <code>TXTReader</code>. This class will inherit from the base <code>BaseReaderConfig</code>:</p> <pre><code>from dataclasses import asdict, dataclass\nfrom yival.data.base_reader import BaseReaderConfig\n\n@dataclass\nclass TXTReaderConfig(BaseReaderConfig):\n    \"\"\"\n    Configuration specific to the TXT reader.\n    \"\"\"\n\n    delimiter: str = \"\\n\"  # Default delimiter for txt files.\n\n    def asdict(self):\n        return asdict(self)\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-reader/#32-implement-the-txtreader-class","title":"3.2. Implement the TXTReader Class","text":"<p>Now, create the <code>TXTReader</code> class, subclassing the <code>BaseReader</code>:</p> <pre><code>from typing import Iterator, List\n\nfrom txt_reader_config import TXTReaderConfig\nfrom yival.data.base_reader import BaseReader\nfrom yival.schemas.common_structures import InputData\n\nclass TXTReader(BaseReader):\n    \"\"\"\n    TXTReader is a class derived from BaseReader to read datasets from TXT\n    files.\n\n    Attributes:\n        config (TXTReaderConfig): Configuration object specifying reader parameters.\n\n    Methods:\n        __init__(self, config: TXTReaderConfig): Initializes the TXTReader with\n        a given configuration.\n        read(self, path: str) -&gt; Iterator[List[InputData]]: Reads the TXT file\n        and yields chunks of InputData.\n    \"\"\"\n\n    config: TXTReaderConfig\n    default_config = TXTReaderConfig()\n\n    def __init__(self, config: TXTReaderConfig):\n        super().__init__(config)\n        self.config = config\n\n    def read(self, path: str) -&gt; Iterator[List[InputData]]:\n        chunk = []\n        chunk_size = self.config.chunk_size\n\n        with open(path, mode=\"r\", encoding=\"utf-8\") as file:\n            for line in file:\n                line_content = line.strip().split(self.config.delimiter)\n\n                # Each line in the TXT file is treated as a separate data point.\n                example_id = self.generate_example_id({\"content\": line_content}, path)\n                input_data_instance = InputData(\n                    example_id=example_id,\n                    content=line_content\n                )\n                chunk.append(input_data_instance)\n\n                if len(chunk) &gt;= chunk_size:\n                    yield chunk\n                    chunk = []\n\n            if chunk:\n                yield chunk\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-reader/#33-config","title":"3.3. Config","text":"<p>After defining the config and reader sublass, we can define the yml config:</p> <pre><code>custom_reader:\n  txt_reader:\n    class: /path/to/text_reader.TXTReader\n    config_cls: /path/to/txt_reader_config.TXTReaderConfig\n</code></pre> <pre><code>dataset:\n  source_type: dataset\n  reader: txt_reader\n  file_path: \"/Users/taofeng/YiVal/data/headline_generation.txt\"\n  reader_config:\n    delimiter: \"\\n\"\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-reader/#4-conclusion","title":"4. Conclusion","text":"<p>Creating custom data readers with the provided framework is straightforward. You can design readers tailored to various data formats by simply subclassing the <code>BaseReader</code> and overriding its <code>read</code> method. With this capability, you can efficiently read data in chunks, making it suitable for parallel processing and large datasets.</p>"},{"location":"eng-spec/custom_parent/custom-selection-strategy/","title":"Custom Selection Strategy Creation Guide","text":""},{"location":"eng-spec/custom_parent/custom-selection-strategy/#introduction","title":"Introduction","text":"<p>Selection strategies are paramount in the experimental framework, guiding the selection or prioritization of experiments, scenarios, or configurations. These strategies can be based on a variety of criteria, ranging from past performance to specific business rules. In this guide, we'll outline the process for creating your own custom selection strategy.</p>"},{"location":"eng-spec/custom_parent/custom-selection-strategy/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>The Essence of Selection Strategy</li> <li>Crafting a Custom Selection Strategy</li> <li>Registering Your Custom Strategy</li> <li>Conclusion</li> </ol>"},{"location":"eng-spec/custom_parent/custom-selection-strategy/#the-essence-of-selection-strategy","title":"The Essence of Selection Strategy","text":"<p>The <code>SelectionStrategy</code> class is the backbone of all selection strategies. It encapsulates core methods to:</p> <ul> <li>Register new selection strategies.</li> <li>Retrieve registered strategies.</li> <li>Access their default configurations.</li> </ul> <p>At its core, a selection strategy's primary task is to decide how to select or prioritize specific experiments or configurations.</p>"},{"location":"eng-spec/custom_parent/custom-selection-strategy/#crafting-a-custom-selection-strategy","title":"Crafting a Custom Selection Strategy","text":"<p>To devise a custom selection strategy, you should inherit from the <code>SelectionStrategy</code> class and implement the <code>select</code> abstract method:</p> <pre><code>class CustomSelectionStrategy(SelectionStrategy):\n    \"\"\"\n    Custom strategy for selecting experiments.\n    \"\"\"\n\n    def select(self, experiment):\n        \"\"\"\n        Custom logic for selecting or prioritizing experiments.\n\n        Args:\n            experiment (Experiment): The experiment under consideration.\n\n        Returns:\n            SelectionOutput: The result of the selection process.\n        \"\"\"\n\n        # Your selection logic goes here\n        pass\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-selection-strategy/#config","title":"Config","text":"<pre><code>custom_selection_strategies:\n  custom_selection_strategy:\n    class: /path/to/custom_selection_strategy.CustomSelectionStrategy\n    config_cls: /path/to/custom_selection_strategy.CustomSelectionStrategyConfig\n</code></pre> <p>To use it</p> <pre><code>selection_strategy:\n  custom_selection_strategies:\n</code></pre>"},{"location":"eng-spec/custom_parent/custom-variation-generator/","title":"Writing a Custom Variation Generator Subclass","text":"<p>This guide explains how to create a custom variation generator subclass based on the <code>BaseVariationGenerator</code> for experimental variations.</p>"},{"location":"eng-spec/custom_parent/custom-variation-generator/#1-understand-the-base","title":"1. Understand the Base","text":"<p>The <code>BaseVariationGenerator</code> provides foundational methods and attributes for all variation generators. Subclasses should implement the <code>generate_variations</code> method to define the logic for producing variations.</p>"},{"location":"eng-spec/custom_parent/custom-variation-generator/#2-example-simplevariationgenerator","title":"2. Example: SimpleVariationGenerator","text":"<p>Let's design a generator that simply returns variations based on the configurations provided.</p>"},{"location":"eng-spec/custom_parent/custom-variation-generator/#21-define-the-configuration-class","title":"2.1 Define the Configuration Class","text":"<p>Firstly, you'll need a configuration class specific to your generator:</p> <pre><code>from dataclasses import dataclass\nfrom yival.schemas.varation_generator_configs import BaseVariationGeneratorConfig\n\n@dataclass\nclass SimpleVariationGeneratorConfig(BaseVariationGeneratorConfig):\n    variations: Optional[List[str]] = None  # List of variations to generate\n</code></pre> <p>This configuration class inherits from <code>BaseVariationGeneratorConfig</code> and has an additional attribute, <code>variations</code>, which is a list of variation strings.</p>"},{"location":"eng-spec/custom_parent/custom-variation-generator/#22-implement-the-variation-generator","title":"2.2 Implement the Variation Generator","text":"<p>Now, let's create the custom variation generator:</p> <pre><code>from typing import Iterator, List\n\nfrom yival.schemas.experiment_config import WrapperVariation\nfrom yival.variation_generators.base_variation_generator import BaseVariationGenerator\n\nclass SimpleVariationGenerator(BaseVariationGenerator):\n\n    def __init__(self, config: SimpleVariationGeneratorConfig):\n        super().__init__(config)\n        self.config = config\n\n    def generate_variations(self) -&gt; Iterator[List[WrapperVariation]]:\n        variations = [WrapperVariation(value_type=\"str\", value=var) for var in self.config.variations]\n        yield variations\n</code></pre> <p>Here, the <code>generate_variations</code> method simply converts the list of variation strings from the configuration into a list of <code>WrapperVariation</code> objects and yields it.</p>"},{"location":"eng-spec/custom_parent/custom-variation-generator/#3-using-the-custom-variation-generator-in-configuration","title":"3. Using the Custom Variation Generator in Configuration","text":"<p>In your configuration (YAML), you can now specify the use of this variation generator:</p> <pre><code>custom_variation_generators:\n  simple_variation_generator:\n    class: /path/to/simple_variation_generator.SimpleVariationGenerator\n    config_cls: /path/to/simple_variation_generator_config.SimpleVariationGeneratorConfig\n</code></pre> <pre><code>variations:\n  - name: task\n    generator_name: simple_variation_generator\n    generator_config:\n      variations:\n        - abc\n        - def\n</code></pre> <p>This configuration will use the <code>SimpleVariationGenerator</code> and produce the variations \"variation1\" and \"variation2\".</p>"},{"location":"eng-spec/custom_parent/custom-wrapper/","title":"Custom Wrapper Creation Guide: NumberWrapper","text":""},{"location":"eng-spec/custom_parent/custom-wrapper/#introduction","title":"Introduction","text":"<p>In the experimental framework, wrappers play a vital role in managing variations throughout an experiment's lifecycle. By creating custom wrappers, one can control and monitor variations tailored to specific needs, ensuring that the experiment operates smoothly and efficiently.</p> <p>In this guide, we will walk you through the process of creating a custom wrapper named <code>NumberWrapper</code>. This wrapper will handle variations specifically for numbers. By the end of this guide, you will have a clear understanding of creating and registering a custom wrapper within the experimental framework.</p>"},{"location":"eng-spec/custom_parent/custom-wrapper/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Base Wrapper Overview</li> <li>Creating a NumberWrapper</li> <li>Registering the NumberWrapper</li> <li>Conclusion</li> </ol>"},{"location":"eng-spec/custom_parent/custom-wrapper/#base-wrapper-overview","title":"Base Wrapper Overview","text":"<p>The <code>BaseWrapper</code> class provides the fundamental structure for wrappers. It comes equipped with methods to register new wrappers, retrieve registered ones, and fetch their configurations. The primary purpose of a wrapper is to manage experiment variations based on the global experiment state.</p>"},{"location":"eng-spec/custom_parent/custom-wrapper/#creating-a-numberwrapper","title":"Creating a NumberWrapper","text":"<p>The <code>NumberWrapper</code> will be a custom wrapper designed to handle variations specifically for numbers.</p> <pre><code>from typing import Optional\n\nfrom number_wrapper_config import NumberWrapperConfig\nfrom yival.wrappers.base_wrapper import BaseWrapper\n\n\nclass NumberWrapper(BaseWrapper):\n    \"\"\"\n    A wrapper for numbers to manage experiment variations based on the global\n    experiment state. If a variation for the given experiment name exists and\n    the global ExperimentState is active, the variation is used. Otherwise,\n    the original number is returned.\n    \"\"\"\n    default_config = NumberWrapperConfig()\n\n    def __init__(\n        self,\n        value: float,\n        name: str,\n        config: Optional[NumberWrapperConfig] = None\n        state: Optional[ExperimentState] = None\n    ) -&gt; None:\n        super().__init__(name, config, state)\n        self._value = value\n\n    def get_value(self) -&gt; float:\n        variation = self.get_variation()\n        if variation is not None:\n            return variation\n        return self._value\n</code></pre> <p>Here, the <code>NumberWrapper</code> class is responsible for retrieving a variation if one exists, otherwise returning the original number. The <code>get_value</code> method is used to fetch the number, considering any variations.</p>"},{"location":"eng-spec/custom_parent/custom-wrapper/#registering-the-numberwrapper","title":"Registering the NumberWrapper","text":"<p>To make the <code>NumberWrapper</code> usable within the experimental framework, it needs to be registered. The registration process involves mapping the wrapper's name to its class and configuration.</p> <pre><code>from dataclasses import dataclass\n\nfrom yival.schemas.wrapper_configs import BaseWrapperConfig\n\n\n@dataclass\nclass NumberWrapperConfig(BaseWrapperConfig):\n    \"\"\"\n    Configuration specific to the NumberWrapper.\n    \"\"\"\n    pass\n</code></pre> <p>By calling the <code>register_wrapper</code> method, the <code>NumberWrapper</code> becomes available for use in the experimental framework.</p>"},{"location":"eng-spec/custom_parent/custom-wrapper/#config","title":"Config","text":"<p>Now you can cofig the wrapper in yml</p> <pre><code>custom_wrappers:\n  number_wrapper:\n    class: /path/to/number_wrapper.NumberWrapper\n    config_cls: /path/to/number_wrapper_config.NumberWrapperConfig\n</code></pre> <p>And you should be able to use the wrapper in your code like string wrapper.</p>"},{"location":"eng-spec/custom_parent/custom-wrapper/#conclusion","title":"Conclusion","text":"<p>By following this guide, you've successfully created and registered a custom wrapper named <code>NumberWrapper</code> in the experimental framework. This flexibility allows you to tailor experiments to specific needs, ensuring accurate and efficient results.</p>"},{"location":"eng-spec/demo/auto-prompts-generation/","title":"Auto Prompts Generation","text":""},{"location":"eng-spec/demo/auto-prompts-generation/#overview","title":"Overview","text":"<p>This document describes the engineering specification auto prompt generation demo</p>"},{"location":"eng-spec/demo/auto-prompts-generation/#flow-diagram","title":"Flow Diagram","text":"<pre><code>flowchart TD\n\n    %% Data Generation Stage\n    A[Start]\n    A --&gt; |Data Generation| A1[Dataset]\n    A1 --&gt; A2[DataGenerator]\n    A2 --&gt; A3[OpenAI Prompt Data Generator]\n\n    %% Create Combinations Stage\n    A1 --&gt; |Create Combinations| B\n    B --&gt; B1[OpenAI Prompt Based Variation Generator]\n    B --&gt; B2[Set Combinations Manually]\n    B --&gt; B3[List of Combinations]\n\n    %% Evaluate Stage\n    B3 --&gt; |Analysis| C\n    C --&gt; C1[User's Function - Headline Generation]\n    C --&gt; C2[Results from Function]\n    A1 --&gt; C1\n\n    %% Evaluator Stage\n    C2 --&gt; |Evaluation| D\n    D --&gt; D1[OpenAI Prompt Based Evaluator: clarity]\n    D --&gt; D2[OpenAI Prompt Based Evaluator: catchiness]\n    D --&gt; D3[OpenAI Prompt Based Evaluator: relevance]\n    D --&gt; D4[OpenAI Elo Evaluator]\n\n    %% Select Stage\n    D --&gt; |Selection| E\n    E --&gt; E1[Selection Strategy]\n    E1 --&gt; E2[AHP Selection Strategy]\n\n    %% Enhancer Stage\n    E --&gt; |Improvement| F\n    F --&gt; F1[OpenAI PromptBased Combination Enhancer]\n    F1 --&gt; C\n\n    %% Styling\n    style A fill:#f9d77e,stroke:#f96e5b\n    style B fill:#a1d4c6,stroke:#f96e5b\n    style C fill:#f6c3d5,stroke:#f96e5b\n    style D fill:#b2b1cf,stroke:#f96e5b\n    style E fill:#f9efaa,stroke:#f96e5b\n    style F fill:#f2a3b3,stroke:#f96e5b</code></pre>"},{"location":"eng-spec/demo/auto-prompts-generation/#specifications","title":"Specifications","text":""},{"location":"eng-spec/demo/auto-prompts-generation/#data-generator","title":"Data Generator","text":"<p>We use open ai to help us generate test cases</p> <pre><code>dataset:\n  data_generators:\n    openai_prompt_data_generator:\n      chunk_size: 1000\n      diversify: false\n      input_function:\n        description:\n          Given an tech startup business, generate a corresponding landing\n          page headline\n        name: headline_generation_for_tech_startup_business\n        parameters:\n          tech_startup_business: str\n      number_of_examples: 2\n      openai_model_name: gpt-4\n      output_path: test_demo_generated_examples.pkl\n      prompt: |-\n        Please provide a concrete and realistic test case as\n        a dictionary for function invocation using the ** operator.\n        Only include parameters, excluding description and name and ensure\n        it's succinct and well-structured.\n\n        **Only provide the dictionary.**\n\n  source_type: machine_generated\n</code></pre> <ul> <li>diversify: Whether to diversify the generated results. If set to true,                  parallel processing of data generation is not possible.</li> <li>input_function: Description and parameters for the function.</li> <li>number_of_examples: Number of examples to be generated.</li> <li>output_path: Temporary output path for the generated data. If this file exists,                    the data will be read from it.</li> <li>prompt: The prompt used to generate the data.</li> </ul>"},{"location":"eng-spec/demo/auto-prompts-generation/#custom-function","title":"Custom Function","text":"<p>The custom function for this mode is hosted on GitHub. You can find and review it here.</p> <p>In this function, we take in an input that represents tech startup business, and will otuput headline for the landing page.</p> <p>We make use of the <code>StringWrapper</code> to enclose placeholders intended for replacement, based on the variations configuration. This wrapped string essentially serves as a namespace:</p> <pre><code>    str(\n        StringWrapper(\n            template=\"\"\"\n            Generate a landing page headline for {tech_startup_business}\n            \"\"\",\n            variables={\n                \"tech_startup_business\": tech_startup_business,\n            },\n            name=\"task\"\n            )\n        )\n</code></pre> <p>Within the <code>StringWrapper</code>, instead of a raw string, we utilize a template. This template integrates both text and variables. The variables are encapsulated within <code>{}</code> braces. In the example provided, the variable is <code>tech_startup_business</code>.</p>"},{"location":"eng-spec/demo/auto-prompts-generation/#variations-generation","title":"Variations Generation","text":"<p>We have configured variations using both ChatGPT and manual settings. Here's the configuration for ChatGPT:</p> <pre><code>variations:\n  - name: task\n    generator_name: openai_prompt_based_variation_generator\n    generator_config:\n      diversify: false\n      max_tokens: 7000\n      number_of_variations: 2\n      openai_model_name: gpt-4\n      output_path: test_demo_generated_prompt.pkl\n      prompt:\n        - content: |- \n            Prompt used to generate the varations. \n</code></pre> <ul> <li>generator_name: Represents the class name of the generator.</li> <li>number_of_variations: Specifies the number of variations to be generated.</li> <li>output_path: This is the temporary storage location for the generated data.                    If a file at this path exists, the data is read from it.</li> <li>prompt: The prompts used to generate the variation.</li> </ul> <p>For manual variations, the configuration is as follows:</p> <pre><code>    variations:\n      - instantiated_value: Generate landing page headline for {tech_startup_business}\n        value: Generate landing page headline for {tech_startup_business}\n        value_type: str\n        variation_id: null\n</code></pre> <p>This manual setup defines a specific way the variation will be presented.</p>"},{"location":"eng-spec/demo/auto-prompts-generation/#human-rating-configuration","title":"Human Rating Configuration","text":"<p>Below is the configuration for human ratings:</p> <pre><code>human_rating_configs:\n\n- name: clarity\n    instructions: Evaluate the clarity of the headline: Does it precisely convey the startup's purpose?\n    scale: [1, 5]\n\n- name: relevance\n    instructions: Assess the relevance of the headline: Is it pertinent to the subject matter?\n    scale: [1, 5]\n</code></pre> <p>Explanation:</p> <ul> <li>name: Specifies the criterion for rating.</li> <li>instructions: Provides guidelines to the rater on how to evaluate the content                     based on the defined criterion.</li> <li>scale: The rating scale, where <code>1</code> is the lowest and <code>5</code> is the highest.</li> </ul> <p>To better understand the configuration, refer to the screenshot below:</p> <p></p> <p>Certainly! Here's the revised description taking into account the context you provided:</p>"},{"location":"eng-spec/demo/auto-prompts-generation/#automated-evaluation-configuration","title":"Automated Evaluation Configuration","text":"<p>Below is the setup for automated evaluations:</p> <pre><code>evaluators:\n  - evaluator_type: all\n    input_description:\n      Given a tech startup business, produce a single corresponding landing\n      page headline\n    name: openai_elo_evaluator\n    openai_model_name: gpt-4\n  - evaluator_type: individual\n    metric_calculators:\n      - method: AVERAGE\n    name: openai_prompt_based_evaluator\n    prompt: |-\n      You're tasked with evaluating an answer for a specific criterion based on the following information:\n      - Task: Given a tech startup business, produce a single corresponding landing page headline\n      - Question: Does the headline clearly convey the startup's mission or the problem it addresses?\n        The headline should instantly provide clarity about the startup's objective to any reader. Ambiguities can deter potential users or investors.\n      [Input]: {tech_startup_business}\n      [Result]: {raw_output}\n      Kindly rate the response based on the following options:\n      A. Doesn't meet the criterion.\n      B. Partially meets the criterion with significant scope for enhancement.\n      C. Adequately meets the criterion.\n      D. Meets the criterion impressively.\n      E. Exemplarily meets the criterion, with negligible areas for improvement.\n    display_name: clarity\n    choices: [\"A\", \"B\", \"C\", \"D\", \"E\"]\n    description: Does the headline lucidly articulate the startup's function or the issue it targets?\n    scale_description: \"0-4\"\n    choice_scores:\n      A: 0\n      B: 1\n      C: 2\n      D: 3\n      E: 4\n</code></pre> <p>Explanation:</p> <ul> <li>evaluator_type: Designates the type of evaluation.<ul> <li><code>all</code>: The evaluator considers all experiment results across all variations.     It uses the elo algorithm and employs GPT-4 as the judge.</li> <li><code>individual</code>: The evaluator focuses solely on the current variation's results.</li> </ul> </li> <li>input_description: Describes the type of input the model expects.</li> <li>name: Represents the evaluator's name or identifier.</li> <li>prompt: Provides the template and context for the automated evaluator     to assess a given result.</li> <li>display_name: Specifies the displayed criterion name on the user interface.</li> <li>choices: Lists all possible rating options for the evaluator.</li> <li>description: Offers a brief description of the evaluation criterion.</li> <li>scale_description: Details the numeric scoring scale.</li> <li>choice_scores: Maps each choice to its respective numeric score.</li> </ul> <p>Certainly! Here's the polished description for the selection configuration:</p>"},{"location":"eng-spec/demo/auto-prompts-generation/#selection-configuration","title":"Selection Configuration","text":"<p>Below is the setup detailing the selection strategy:</p> <pre><code>selection_strategy:\n  ahp_selection:\n    criteria:\n      - openai_elo_evaluator\n      - average_token_usage\n      - average_latency\n      - \"openai_prompt_based_evaluator: clarity\"\n      - \"openai_prompt_based_evaluator: relevance\"\n      - \"openai_prompt_based_evaluator: catchiness\"\n    criteria_maximization:\n      openai_elo_evaluator: true\n      average_latency: false\n      average_token_usage: false\n    criteria_weights:\n      openai_elo_evaluator: 0.3\n      average_latency: 0.2\n      average_token_usage: 0.2\n      \"openai_prompt_based_evaluator: clarity\": 0.1\n      \"openai_prompt_based_evaluator: relevance\": 0.1\n      \"openai_prompt_based_evaluator: catchiness\": 0.1\n</code></pre> <p>Explanation:</p> <ul> <li>selection_strategy: Represents the overarching approach for making selections.</li> <li>ahp_selection: Specifies that the Analytic Hierarchy Process (AHP) algorithm     is employed for the selection strategy.</li> <li>criteria: Lists the evaluators and metrics that are considered during     the selection process.</li> <li>criteria_maximization: Indicates whether each criterion should be maximized.     For instance,     while a high score from the <code>openai_elo_evaluator</code> is desirable (<code>true</code>),     a lower <code>average_latency</code> or <code>average_token_usage</code> is preferred (<code>false</code>).</li> <li>criteria_weights:     Assigns a weight to each criterion,     determining its importance in the overall evaluation.     The weights sum up to 1, indicating the relative significance of each criterion     in the final decision-making process.</li> </ul>"},{"location":"eng-spec/demo/auto-prompts-generation/#auto-enhancer","title":"Auto Enhancer","text":"<p>Certainly! Here's the enhanced description, incorporating the additional information:</p>"},{"location":"eng-spec/demo/auto-prompts-generation/#auto-enhancer-configuration","title":"Auto Enhancer Configuration","text":"<p>Below is the setup detailing the auto enhancer strategy:</p> <pre><code>enhancer:\n  name: openai_prompt_based_combination_enhancer\n  max_iterations: 2\n  openai_model_name: gpt-4\n  stop_conditions:\n    \"openai_prompt_based_evaluator: catchiness\": 3\n    \"openai_prompt_based_evaluator: clarity\": 3\n    \"openai_prompt_based_evaluator: relevance\": 3\n</code></pre> <p>Explanation:</p> <ul> <li>name: Specifies the identifier or the class of the enhancer.     In this instance, <code>openai_prompt_based_combination_enhancer</code> is utilized.</li> <li>max_iterations: Designates the upper limit for the number of improvement cycles.     The process will not exceed 2 iterations, irrespective of other conditions.</li> <li>openai_model_name:     Indicates the model to be utilized for the improvement process,     which here is <code>gpt-4</code>.</li> <li>stop_conditions:     Outlines conditions under which the Enhancer should halt its operations     before reaching the maximum iteration count.     The improvement process will terminate if the average score     from any of the specified evaluators surpasses 3.</li> </ul> <p>Additionally, it's important to note that during the improvement process, only the best result from the previous selection step will be taken into consideration. The idea is to refine and optimize this top-performing result further.</p>"},{"location":"eng-spec/demo/auto-prompts-generation/#full-configuration","title":"Full Configuration","text":"<p>For a comprehensive view of all configurations related to the basic interactive mode, you can review the full configuration file hosted here.</p>"},{"location":"eng-spec/demo/basic-interactive-mode/","title":"Basic Interactive Mode","text":""},{"location":"eng-spec/demo/basic-interactive-mode/#overview","title":"Overview","text":"<p>This document describes the engineering specification for the basic interactive mode in the YiVal framework.</p>"},{"location":"eng-spec/demo/basic-interactive-mode/#flow-diagram","title":"Flow Diagram","text":"<pre><code>flowchart TD\n\n    %% Data Generation Stage (simplified)\n    A[Start]\n    A --&gt; |Data Generation| A1[Manual Input]\n\n    %% Create Combinations Stage (simplified)\n    A1 --&gt; |Create Combinations| B\n    B --&gt; B1[Set Combinations Manually]\n\n    %% Evaluate Stage (simplified)\n    B1 --&gt; |Analysis| C\n    C --&gt; C1[User's Function]\n    A1 --&gt; C1\n    C --&gt; C2[Results from Function]\n\n    %% Styling\n    style A fill:#f9d77e,stroke:#f96e5b\n    style B fill:#a1d4c6,stroke:#f96e5b\n    style C fill:#f6c3d5,stroke:#f96e5b\n</code></pre>"},{"location":"eng-spec/demo/basic-interactive-mode/#specifications","title":"Specifications","text":""},{"location":"eng-spec/demo/basic-interactive-mode/#data-source","title":"Data Source","text":"<p>The data for this mode comes directly from the user input. The configuration for this data source is as follows:</p> <pre><code>dataset:\n  source_type: user_input\n</code></pre>"},{"location":"eng-spec/demo/basic-interactive-mode/#custom-function","title":"Custom Function","text":"<p>The custom function for this mode is hosted on GitHub. You can find and review it here.</p> <p>Within this function, we utilize the <code>StringWrapper</code> to wrap places that will be replaced based on the variations configuration. The wrapped string acts as a namespace:</p> <pre><code>str(\n    StringWrapper(\n        \"Translate the following to Chinese\", name=\"translate\"\n    )\n) + f'{input}'\n</code></pre>"},{"location":"eng-spec/demo/basic-interactive-mode/#variations-configuration","title":"Variations Configuration","text":"<p>The variations are defined in a configuration that provides multiple instantiated values, each corresponding to a different language. Here's a snapshot:</p> <pre><code>variations:\n  - name: translate\n    variations:\n      - instantiated_value: \"Translate the following to Chinese:\"\n        value: \"Translate the following to Chinese\"\n        value_type: str\n        variation_id: null\n      - instantiated_value: \"Translate the following to Spanish:\"\n        value: \"Translate the following to Spanish\"\n        value_type: str\n        variation_id: null\n      - instantiated_value: \"Translate the following to German:\"\n        value: \"Translate the following to German\"\n        value_type: str\n        variation_id: null\n</code></pre> <p>This configuration provides three variations, allowing the text to be set in different languages.</p>"},{"location":"eng-spec/demo/basic-interactive-mode/#full-configuration","title":"Full Configuration","text":"<p>For a comprehensive view of all configurations related to the basic interactive mode, you can review the full configuration file hosted here.</p>"},{"location":"eng-spec/demo/question-answering-with-expected-result/","title":"Question Answering with Expected Results","text":""},{"location":"eng-spec/demo/question-answering-with-expected-result/#overview","title":"Overview","text":"<p>This documentation provides an insight into the basic reader and evaluator components within the Yival framework.</p>"},{"location":"eng-spec/demo/question-answering-with-expected-result/#flow-diagram","title":"Flow Diagram","text":"<p>To better understand the process, refer to the flow diagram below:</p> <pre><code>flowchart TD\n\n    %% Data Generation Stage\n    A[Start]\n    A --&gt; |Data Generation| A1[Dataset]\n    A1 --&gt; A2[DataReader]\n    A2 --&gt; A3[CSVReader]\n    A1 --&gt; A6[Manual Input]\n\n    %% Create Combinations Stage\n    A1 --&gt; |Create Combinations| B\n    B --&gt; B2[Set Combinations Manually]\n\n    %% Evaluate Stage\n    B2 --&gt; |Analysis| C\n    C --&gt; C1[User's Function]\n    C --&gt; C2[Results from Function]\n\n    %% Evaluator Stage\n    C2 --&gt; |Evaluation| D\n    D --&gt; D3[String_Expected_results_Evaluator]\n\n    %% Styling\n    style A fill:#f9d77e,stroke:#f96e5b\n    style B fill:#a1d4c6,stroke:#f96e5b\n    style C fill:#f6c3d5,stroke:#f96e5b\n    style D fill:#b2b1cf,stroke:#f96e5b</code></pre>"},{"location":"eng-spec/demo/question-answering-with-expected-result/#specifications","title":"Specifications","text":""},{"location":"eng-spec/demo/question-answering-with-expected-result/#data-source","title":"Data Source","text":"<p>The data for this example is sourced from a CSV file that includes an <code>expected_result</code> column. Here's the configuration for the data source:</p> <pre><code>dataset:\n  file_path: demo/data/yival_expected_results.csv\n  reader: csv_reader\n  source_type: dataset\n  reader_config:\n    expected_result_column: expected_result\n</code></pre>"},{"location":"eng-spec/demo/question-answering-with-expected-result/#custom-function","title":"Custom Function","text":"<p>Our custom function tailored for this mode is hosted on GitHub. To delve into its details, please click here.</p> <p>In this function, the <code>StringWrapper</code> is employed to encapsulate parts of the string that will be substituted based on the variations configuration. This encapsulated string serves as a namespace:</p> <pre><code>    \"content\": f'{input} ' + str(StringWrapper(\"\", name=\"qa\"))\n</code></pre>"},{"location":"eng-spec/demo/question-answering-with-expected-result/#variations-configuration","title":"Variations Configuration","text":"<p>Variations are articulated in a dedicated configuration. This particular setup allows retaining the original empty suffix or appends a \"Chain of Thought\" suffix:</p> <pre><code>variations:\n  - name: qa\n    variations:\n      - instantiated_value: \"\"\n        value: \"\"\n        value_type: str\n        variation_id: null\n      - instantiated_value: \"Think first, then make a decision. Some random thoughts:\"\n        value: \"Think first, then make a decision. Some random thoughts:\"\n        value_type: str\n        variation_id: null\n</code></pre>"},{"location":"eng-spec/demo/question-answering-with-expected-result/#full-configuration","title":"Full Configuration","text":"<p>For those seeking an in-depth exploration of all configurations pertinent to the QA expected results demonstration, we recommend perusing the comprehensive configuration file available here.</p>"}]}